{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Assistants - File Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create a client\n",
    "client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "vector_store = client.vector_stores.create(name=\"Nasa Books\")\n",
    "\n",
    "# Specify the folder containing the files\n",
    "folder_path = \"../Data/nasabooks/\"\n",
    "\n",
    "# Get all file paths in the folder\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)]\n",
    "\n",
    "# Open file streams\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=28, failed=0, in_progress=0, total=28)\n",
      "vs_HNVzCmkwhPs3sgBupMB1YH6T\n"
     ]
    }
   ],
   "source": [
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "print(vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat citations with the proper filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_citations(content_block):\n",
    "    # Extract the annotations\n",
    "    annotations = content_block.text.annotations\n",
    "    \n",
    "    # Original response\n",
    "    paragraph = content_block.text.value\n",
    "\n",
    "    # Dictionary to store key-value pairs of text and filename\n",
    "    text_filename_pairs = {}\n",
    "\n",
    "    # Iterate over the annotations and extract the relevant information\n",
    "    for annotation in annotations:\n",
    "        file_id = annotation.file_citation.file_id\n",
    "        text = annotation.text\n",
    "        cited_file = client.files.retrieve(file_id)\n",
    "        filename = cited_file.filename\n",
    "\n",
    "        if text not in text_filename_pairs:\n",
    "            text_filename_pairs[text] = []\n",
    "        text_filename_pairs[text].append(filename)\n",
    "\n",
    "    # Replace the citation texts with their corresponding filenames prefixed with \" Source: \"\n",
    "    for text, filenames in text_filename_pairs.items():\n",
    "        sources = \" Source: \" + \", \".join(filenames)\n",
    "        paragraph = paragraph.replace(text, sources)\n",
    "\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1-2:\n",
    "1. Create an Assistant\n",
    "2. Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_NItTCRsavmsJAYr5mZ36eG62', created_at=1750534046, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Nasa books Assistant\",\n",
    "  instructions=\"\"\"\n",
    "  You are a assistant that provides information. \n",
    "   You will answer questions based on files provided to you about information in a NASA Book. \n",
    "   You will not provide answers outside of those files.\n",
    "  \"\"\",\n",
    "  model=azure_openai_deployment,\n",
    "  tools=[{\"type\":\"file_search\"}],\n",
    "  tool_resources={\"file_search\":{\"vector_store_ids\":[vector_store.id]}},\n",
    "  temperature=1,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "# Step 2: Create thread\n",
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-6: Helper Function\n",
    "3. Add a message to the thread\n",
    "4. Run the Assistant\n",
    "5. Check the Run Status\n",
    "6. Display the Assistant's Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_assistant(user_question):\n",
    "  # Step 3: Add a message to the thread\n",
    "  messages = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=user_question\n",
    "  )\n",
    "\n",
    "  # Step 4: Run the Assistant\n",
    "  run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    "  )\n",
    "\n",
    "  # Step 5: Check the Run Status\n",
    "  # Looping until the run completes or fails\n",
    "  while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "      thread_id=thread.id,\n",
    "      run_id=run.id\n",
    "    )\n",
    "\n",
    "    #display run status\n",
    "    print(run.status)\n",
    "\n",
    "    if run.status == 'completed':\n",
    "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "      # Step 6: Display the Assistant's Response\n",
    "      content_block = messages.data[0].content[0]\n",
    "      annotations = content_block.text.annotations\n",
    "      if annotations is None:\n",
    "        value = content_block.text.value\n",
    "        print(value)\n",
    "      else:\n",
    "        print(reformat_citations(content_block))\n",
    "    \n",
    "    elif run.status == 'requires_action':\n",
    "      pass\n",
    "    \n",
    "    elif run.status == \"failed\":\n",
    "        print(run.last_error)\n",
    "        \n",
    "    else:\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_progress\n",
      "in_progress\n",
      "completed\n",
      "The wide floodplains in Queensland, specifically in Australia's Channel Country, originated from the extreme variation in water and sediment discharges from the rivers. The region is characterized by flat land and poor drainage, which allows for the formation of semi-permanent wetlands at river convergence points. In many years, rainfall is scarce, resulting in rivers that are nearly nonexistent, while in wetter years, high discharges can inundate the entire width of the floodplain. These occasional massive flows can transform the floodplain into extensive areas of water, with only treetops visible above the surface Source: page-49.pdf.\n"
     ]
    }
   ],
   "source": [
    "user_question =\"\"\"How did the wide floodplains in Queensland originate?\"\"\"\n",
    "run_assistant(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_progress\n",
      "completed\n",
      "The Lower Amazon River is formed at the \"Meeting of the Waters,\" where two significant rivers converge: the coffee-colored Rio Solimões and the black-tea-colored Rio Negro. The Rio Solimões, which is rich in sediment, flows down from the Andes Mountains, while the nearly sediment-free Rio Negro flows from the Colombian hills and jungles, gaining its color from decayed leaf and plant matter. The two rivers run side by side for several kilometers before they mix and form the Lower Amazon River Source: page-61.pdf.\n"
     ]
    }
   ],
   "source": [
    "user_question =\"\"\"What forms the Lower Amazon River?\"\"\"\n",
    "run_assistant(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.assistants.delete(assistant.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Vector Store to avoid storage costs on Azure. It incur costs after first 1GB free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreDeleted(id='vs_HNVzCmkwhPs3sgBupMB1YH6T', deleted=True, object='vector_store.deleted')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.vector_stores.delete(vector_store.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
