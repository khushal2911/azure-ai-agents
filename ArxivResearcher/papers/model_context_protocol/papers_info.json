{
  "2506.19852v1": {
    "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
    "authors": [
      "Xingyang Li",
      "Muyang Li",
      "Tianle Cai",
      "Haocheng Xi",
      "Shuo Yang",
      "Yujun Lin",
      "Lvmin Zhang",
      "Songlin Yang",
      "Jinbo Hu",
      "Kelly Peng",
      "Maneesh Agrawala",
      "Ion Stoica",
      "Kurt Keutzer",
      "Song Han"
    ],
    "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with $O(n \\log\nn)$ complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard $O(n^2)$ dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4$\\times$ longer while reducing training costs by up to\n4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to\n3.7$\\times$ compared to dense attention inference.",
    "pdf_url": "http://arxiv.org/pdf/2506.19852v1",
    "published": "2025-06-24"
  },
  "2506.19851v1": {
    "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models",
    "authors": [
      "Zehuan Huang",
      "Haoran Feng",
      "Yangtian Sun",
      "Yuanchen Guo",
      "Yanpei Cao",
      "Lu Sheng"
    ],
    "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\n\\href{https://anima-x.github.io/}{https://anima-x.github.io/}.",
    "pdf_url": "http://arxiv.org/pdf/2506.19851v1",
    "published": "2025-06-24"
  },
  "2506.19850v1": {
    "title": "Unified Vision-Language-Action Model",
    "authors": [
      "Yuqi Wang",
      "Xinghang Li",
      "Wenxuan Wang",
      "Junbo Zhang",
      "Yingyan Li",
      "Yuntao Chen",
      "Xinlong Wang",
      "Zhaoxiang Zhang"
    ],
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
    "pdf_url": "http://arxiv.org/pdf/2506.19850v1",
    "published": "2025-06-24"
  },
  "2506.19847v1": {
    "title": "Orthogonal Finetuning Made Scalable",
    "authors": [
      "Zeju Qiu",
      "Weiyang Liu",
      "Adrian Weller",
      "Bernhard Sch\u00f6lkopf"
    ],
    "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage.",
    "pdf_url": "http://arxiv.org/pdf/2506.19847v1",
    "published": "2025-06-24"
  },
  "2506.19846v1": {
    "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning",
    "authors": [
      "Ai Han",
      "Junxing Hu",
      "Pu Wei",
      "Zhiqian Zhang",
      "Yuhang Guo",
      "Jiawei Lu",
      "Zicheng Zhang"
    ],
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models.",
    "pdf_url": "http://arxiv.org/pdf/2506.19846v1",
    "published": "2025-06-24"
  }
}