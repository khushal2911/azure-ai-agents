{
  "2506.19852v1": "arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\nRadial Attention: O(n log n) Sparse Attention\nwith Energy Decay for Long Video Generation\nXingyang Li\u2217 Muyang Li\u2217 Tianle Cai Haocheng Xi\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\nhttps://github.com/mit-han-lab/radial-attention\n\u201cNothing spreads without loss; every signal, every influence, every attention \u2014\ndecays with distance.\u201d \u2014 Inspired by thermodynamic principles\nPre-Trained\nLoRA-Tuned Pre-Trained\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9\u00d7 Faster)    PSNR: 27.3 \n(a) 117 Frames (Default Length)\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\nDense Attention    Vision Reward: 0.133 \nTuning Cost: 746 GPU Hours      Latency: 2895s\nRadial Attention (Ours)    Vision Reward: 0.134 \nTuning Cost: 171 GPU Hours (4.4\u00d7 Fewer)     Latency: 781s (3.7\u00d7 Faster)\n(b) 509 Frames (4\u00d7 Extension)\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9\u00d7 at its default video length while maintaining\ncomparable video quality. When generating 4\u00d7 longer videos, it reduces tuning costs by up to 4.4\u00d7 and speeds\nup inference by up to 3.7\u00d7 versus dense attention.\nAbstract\nRecent advances in diffusion models have enabled high-quality video generation,\nbut the additional temporal dimension significantly increases computational costs,\nmaking training and inference on long videos prohibitively expensive. In this\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\ndistance between tokens increase, akin to the physical decay of signal or waves\nover space and time in nature. Motivated by this, we propose Radial Attention, a\nscalable sparse attention mechanism with O(n log n) complexity that translates\nenergy decay into exponentially decaying compute density, which is significantly\nmore efficient than standard O(n2) dense attention and more expressive than linear\nattention. Specifically, Radial Attention employs a simple, static attention mask\nwhere each token attends to spatially nearby tokens, with the attention window size\n\u2217indicates equal contributions.\nPreprint. Under review.",
  "2506.19851v1": "arXiv:2506.19851v1  [cs.CV]  24 Jun 2025\nAnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\nModels\nZEHUAN HUANG, Beihang University, China\nHAORAN FENG, Tsinghua University, China\nYANGTIAN SUN, The University of Hong Kong, China\nYUANCHEN GUO\u2217, VAST, China\nYANPEI CAO\u2020, VAST, China\nLU SHENG\u2020, Beihang University, China\nFig. 1. Diverse articulated 3D models animated using AnimaX. The created animation, spanning various categories including humanoids, animals, and fictional\nmodels, demonstrates the versatility of our method. Selected models are visualized with keyframes of their predicted animations on the conveyor belts.\nWe present AnimaX, a feed-forward 3D animation framework that bridges\nthe motion priors of video diffusion models with the controllable structure\nof skeleton-based animation. Traditional motion synthesis methods are\neither restricted to fixed skeletal topologies or require costly optimization\nin high-dimensional deformation spaces. In contrast, AnimaX effectively\ntransfers video-based motion knowledge to the 3D domain, supporting\ndiverse articulated meshes with arbitrary skeletons. Our method represents\n\u2217Project leader.\n\u2020Corresponding author.\nAuthors\u2019 addresses: Zehuan Huang, Beihang University, China, huangzehuan@buaa.\nedu.cn; Haoran Feng, Tsinghua University, China, fenghr24@mails.tsinghua.edu.cn;\nYangtian Sun, The University of Hong Kong, China, sunyangtian98@gmail.com;\nYuanchen Guo, VAST, China, imbennyguo@gmail.com; Yanpei Cao, VAST, China,\ncaoyanpei@gmail.com; Lu Sheng, Beihang University, China, lsheng@buaa.edu.cn.\n3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-\npose diffusion conditioned on template renderings and a textual motion\nprompt. We introduce shared positional encodings and modality-aware\nembeddings to ensure spatial-temporal alignment between video and pose\nsequences, effectively transferring video priors to motion generation task.\nThe resulting multi-view pose sequences are triangulated into 3D joint\npositions and converted into mesh animation via inverse kinematics. Trained\non a newly curated dataset of 160,000 rigged sequences, AnimaX achieves\nstate-of-the-art results on VBench in generalization, motion fidelity, and\nefficiency, offering a scalable solution for category-agnostic 3D animation.\nProject page: https://anima-x.github.io/.\nAdditional Key Words and Phrases: 3D animation generation, generative\nmodel, 4D generation",
  "2506.19850v1": "arXiv:2506.19850v1  [cs.CV]  24 Jun 2025\nUnified Vision-Language-Action Model\nYuqi Wang1\u2217 Xinghang Li2 Wenxuan Wang1,2 Junbo Zhang3 Yingyan Li1\nYuntao Chen4 Xinlong Wang2\u0000 Zhaoxiang Zhang1\u0000\n1 CASIA 2 BAAI 3 THU 4 HKISI\nProject page: https://robertwyq.github.io/univla.github.io\nVIT\nLanguage Model\nAction output\nMulti-modal Model\nMulti-modal output\nUnified Post-train Tasks\nWorld ModelPerception Grounding\nUnified Vision-Language-Action Model\nInstruction: pick sponge \nfrom top drawer and \nplace on counter\nInstruction: locate the \nposition of the fork\nText Supervision\nInterleaved VideoStatic Image\nLIBERO\nBRIDGE\nPolicy Learning\nInstruction: Put \nspoon on the towel\nVision Supervision\n Action Supervision\n[x, y, z, roll, pitch, yaw, gripper]\n[x1,y1,x2,y2]\n[0. 1, 0. 2, 0. 1, 0. 01, 0. 01, 0. 0, 0. 5]\n[0. 1,-0.1,0.1,0.02,0.01,0.2,0.8]\nCALVIN\nFigure 1: We present UniVLA, a unified vision-language-action model. Unlike prior VLA\napproaches that typically rely on an extra vision encoder to extract image features and generate\nonly action outputs, UniVLA represents vision, language, and action as discrete tokens within a\nunified autoregressive framework. This unified modeling paradigm enables multi-modal outputs and\nsupports a wide range of tasks\u2014such as text-supervised perception grounding, vision-supervised\nworld modeling, and action-supervised policy learning\u2014within a single architecture. The unified\ntoken-based design further allows UniVLA to effectively leverage large-scale multimodal data,\nparticularly video, for scalable and generalizable learning. UniVLA achieves new state-of-the-art\nresults on CALVIN, LIBERO, and SimplerEnv-Bridge, significantly surpassing existing methods.\nAbstract\nVision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This formulation\nenables flexible multimodal tasks learning, particularly from large-scale video\ndata. By incorporating world modeling during post-training, UniVLA captures\ncausal dynamics from videos, facilitating effective transfer to downstream policy\nlearning\u2014especially for long-horizon tasks. Our approach sets new state-of-the-art\nresults across several widely used simulation benchmarks, including CALVIN,\nLIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For\n\u2217Work done during an internship at BAAI. \u0000 Corresponding author\nPreprint. Under review.",
  "2506.19847v1": "arXiv:2506.19847v1  [cs.LG]  24 Jun 2025\nOrthogonal Finetuning Made Scalable\nZeju Qiu1,\u2020 Weiyang Liu1,2,\u2020,* Adrian Weller3,4 Bernhard Sch\u00f6lkopf1\n1Max Planck Institute for Intelligent Systems 2The Chinese University of Hong Kong\n3University of Cambridge 4The Alan Turing Institute \u2020Equal contribution\n*Project lead, Correspondence to wyliu@cse.cuhk.edu.hk\nspherelab.ai/oftv2\nAbstract\nOrthogonal finetuning (OFT) offers highly\nparameter-efficient adaptation while preventing\ncatastrophic forgetting, but its high runtime and\nmemory demands limit practical deployment.\nWe identify the core computational bottleneck\nin OFT as its weight-centric implementation,\nwhich relies on costly matrix-matrix multiplica-\ntions with cubic complexity. To overcome this,\nwe propose OFTv2, an input-centric reformula-\ntion that instead uses matrix-vector multiplica-\ntions (i.e., matrix-free computation), reducing\nthe computational cost to quadratic. We further\nintroduce the Cayley\u2013Neumann parameteriza-\ntion, an efficient orthogonal parameterization\nthat approximates the matrix inversion in Cay-\nley transform via a truncated Neumann series.\nThese modifications allow OFTv2 to achieve up\nto 10\u00d7 faster training and 3\u00d7 lower GPU mem-\nory usage without compromising performance.\nIn addition, we extend OFTv2 to support fine-\ntuning quantized foundation models and show\nthat it outperforms the popular QLoRA in train-\ning stability, efficiency, and memory usage.\n1 Introduction\nAs foundation models continue to improve in per-\nformance, recent years have witnessed a paradigm\nshift from end-to-end learning to a pretraining-\nfinetuning framework. This shift underscores the\nneed for finetuning methods that are both effec-\ntive and scalable. Owing to its training stabil-\nity and adaptation efficiency, orthogonal finetun-\ning (OFT) (Qiu et al., 2023; Liu et al., 2024) has\nemerged as a promising approach for adapting\nfoundation models to downstream tasks. However,\nwhile performing well, OFT incurs high compu-\ntational and memory costs, limiting its scalability.\nMotivated by these challenges, we seek to make\nOFT more scalable to large foundation models.\nTowards this goal, we begin by identifying the\nkey bottleneck that limits OFT\u2019s scalability. At\nOFT OFTv2\n0\n20\n40\n60\n80GPU memory (GB)\nOFT OFTv2\n0\n100\n200\n300Training time (s) / 100 iterations\n>3x >10x\nFigure 1: OFTv2 significantly reduces training time and\nGPU memory usage without sacrificing performance.\nThe finetuning is performed with Qwen2.5-7B.\nits core, OFT learns layer-shared orthogonal ma-\ntrices to transform pretrained weight matrices, re-\nsulting in a naive weight-centric implementation\nwhere forward inference is performed after merg-\ning the learned orthogonal matrices into weight\nmatrices during training. The weight-centric im-\nplementation thus involves matrix-matrix multipli-\ncations with cubic complexity. As weight matri-\nces grow large, this cubic scaling severely limits\nOFT\u2019s applicability to large foundation models.\nHowever, these matrix-matrix multiplications are\nnot fundamentally necessary. We draw inspiration\nfrom matrix-free methods (Chen, 2005), such as the\npower method and the Lanczos algorithm, which\navoid explicit matrix-matrix operations by treat-\ning matrices as linear operators applied to vectors.\nThese methods operate entirely through matrix-\nvector multiplications, applying a matrix to vectors\nin the appropriate space without ever forming full\nmatrix products. Guided by the same insight, we\nintroduce an input-centric implementation of OFT,\nin which the learned orthogonal transformations\nare applied directly to the input vectors during each\nforward pass, rather than being merged into the\nweight matrix. This reformulation reduces the com-\nplexity from cubic to quadratic. We refer to this\nnew formulation as OFTv2. Despite its simplicity,\nthis change significantly enhances the scalability of\n1",
  "2506.19846v1": "arXiv:2506.19846v1  [cs.AI]  24 Jun 2025\nJoyAgents-R1: Joint Evolution Dynamics for Versatile\nMulti-LLM Agents with Reinforcement Learning\nAi Han*\u2020, Junxing Hu*, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang\nJD.com\nBeijing, China\nAbstract\nMulti-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training instability.\nIn this paper, we propose the joint evolution dynamics for MARL called JoyAgents-\nR1, which first applies Group Relative Policy Optimization (GRPO) to the joint\ntraining of heterogeneous multi-agents. By iteratively refining agents\u2019 large lan-\nguage models (LLMs) and memories, the method achieves holistic equilibrium\nwith optimal decision-making and memory capabilities. Specifically, JoyAgents-R1\nfirst implements node-wise Monte Carlo sampling on the behavior of each agent\nacross entire reasoning trajectories to enhance GRPO sampling efficiency while\nmaintaining policy diversity. Then, our marginal benefit-driven selection strat-\negy identifies top-K sampling groups with maximal reward fluctuations, enabling\ntargeted agent model updates that improve training stability and maximize joint\nbenefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-\nR1 introduces an adaptive memory evolution mechanism that repurposes GRPO\nrewards as cost-free supervisory signals to eliminate repetitive reasoning and ac-\ncelerate convergence. Experiments across general and domain-specific scenarios\ndemonstrate that JoyAgents-R1 achieves performance comparable to that of larger\nLLMs while built on smaller open-source models.\n1 Introduction\nThe rapid advancement of Large Language Models (LLMs) [1\u20135] has revolutionized agent-based\nsystems, empowering agents with the ability to perform reasoning, planning, and natural language\ninteraction across diverse domains [6]. Compared to single agents with specialized functionalities [7\u2013\n10], multi-agent systems demonstrate superior flexibility and scalability in tackling complicated tasks\nsuch as Artificial Intelligence (AI) assistants [11] and emergency responder management [12], among\nwhich Multi-Agent Reinforcement Learning (MARL) is a mainstream subfield of the community [13].\nSince Reinforcement Learning (RL) has demonstrated remarkable efficacy in aligning models with\nhuman preferences [14], LLM-based MARL methods have flourished and achieved certain results in\nsophisticated task decomposition [15, 16] and adaptive coordination [17, 18]. In MARL, the behavior\nof one agent may affect the rewards of other agents, which may cause environmental instability and\nlead to low system efficiency and performance [19]. While methods like Multi-Agent Proximal Policy\nOptimization (MAPPO) [20] have advanced MARL by adapting PPO [21] to multi-agent settings,\ntheir reliance on additional value functions introduces critical limitations. Moreover, the decoupling\nof policy and value updates in actor-critic architecture often leads to training instability, particularly\nwhen coordinating heterogeneous agents with misaligned reward structures [22], which poses severe\nchallenges to the dynamics of multi-agent evolution.\n*Equal contribution. \u2020Corresponding author: <hanai5@jd.com>.\nPreprint. Under review."
}