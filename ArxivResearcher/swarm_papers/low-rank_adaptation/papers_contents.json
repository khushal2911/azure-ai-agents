{
  "2506.20671v1": "arXiv:2506.20671v1  [cs.CV]  25 Jun 2025\nIPFormer: Visual 3D Panoptic Scene Completion\nwith Context-Adaptive Instance Proposals\nMarkus Gross1,2,\u2217\nAya Fahmy1 Danit Niwattananan2\nDominik Muhle2 Rui Song1,2 Daniel Cremers2 Henri Mee\u00df1\n1Fraunhofer Institute IVI 2Technical University of Munich\nAbstract\nSemantic Scene Completion (SSC) has emerged as a pivotal approach for jointly\nlearning scene geometry and semantics, enabling downstream applications such as\nnavigation in mobile robotics. The recent generalization to Panoptic Scene Com-\npletion (PSC) advances the SSC domain by integrating instance-level information,\nthereby enhancing object-level sensitivity in scene understanding. While PSC was\nintroduced using LiDAR modality, methods based on camera images remain largely\nunexplored. Moreover, recent Transformer-based SSC approaches utilize a fixed set\nof learned queries to reconstruct objects within the scene volume. Although these\nqueries are typically updated with image context during training, they remain static\nat test time, limiting their ability to dynamically adapt specifically to the observed\nscene. To overcome these limitations, we propose IPFormer, the first approach\nthat leverages context-adaptive instance proposals at train and test time to address\nvision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively\ninitializes these queries as panoptic instance proposals derived from image context\nand further refines them through attention-based encoding and decoding to reason\nabout semantic instance-voxel relationships. Experimental results show that our\napproach surpasses state-of-the-art methods in overall panoptic metrics PQ\u2020 and\nPQ-All, matches performance in individual metrics, and achieves a runtime reduc-\ntion exceeding 14\u00d7. Furthermore, our ablation studies reveal that dynamically\nderiving instance proposals from image context, as opposed to random initializa-\ntion, leads to a 3.62 %increase in PQ-All and a remarkable average improvement\nof 18.65 %in combined Thing-metrics. These results underscore the effectiveness\nof IPFormer and highlight its introduction of context-adaptive instance proposals\nas a pioneering effort in addressing vision-based 3D Panoptic Scene Completion.\n1 Introduction\nPanoptic scene completion provides a holistic scene understanding that can serve applications like\nautonomous driving and robotics by reconstructing volumetric geometry from sparse sensor data\nand assigning meaning to objects in the scene. Such holistic scene understanding requires both\naccurate geometry and semantic information about a scene. Historically, these tasks have been treated\nseparately and evolved independently due to the distinct nature of geometric reconstruction and\nsemantic interpretation, each requiring specialized algorithms and data representations. Despite the\ninherent interconnection between geometry and semantics, their separation hindered unified scene\nunderstanding. Classical structure-from-motion evolved into simultaneous localization and mapping\n(SLAM) systems for accurate reconstructions [36, 12, 25], while deep learning facilitated monocular\ndepth estimation (MDE) methods [14, 15, 50], which are restricted to visible surfaces. In contrast,\nscene completion refers to the process of inferring the complete 3D structure of a scene, including\nboth visible and occluded regions, from partial observations [49, 16].\n\u2217Corresponding author: markus.gross@tum.de",
  "2506.20668v1": "arXiv:2506.20668v1  [cs.RO]  25 Jun 2025\nDemoDiffusion: One-Shot Human Imitation\nusing pre-trained Diffusion Policy\nSungjae Park, Homanga Bharadhwaj, Shubham Tulsiani\nCarnegie Mellon University\n{sungjae2, hbharadh, stulsian}@andrew.cmu.edu\nAbstract: We proposeDemoDiffusion, a simple and scalable method for enabling\nrobots to perform manipulation tasks in natural environments by imitating a sin-\ngle human demonstration. Our approach is based on two key insights. First, the\nhand motion in a human demonstration provides a useful prior for the robot\u2019s end-\neffector trajectory, which we can convert into a rough open-loop robot motion\ntrajectory via kinematic retargeting. Second, while this retargeted motion cap-\ntures the overall structure of the task, it may not align well with plausible robot\nactions in-context. To address this, we leverage a pre-trained generalist diffusion\npolicy to modify the trajectory, ensuring it both follows the human motion and re-\nmains within the distribution of plausible robot actions. Our approach avoids the\nneed for online reinforcement learning or paired human-robot data, enabling ro-\nbust adaptation to new tasks and scenes with minimal manual effort. Experiments\nin both simulation and real-world settings show that DemoDiffusion outperforms\nboth the base policy and the retargeted trajectory, enabling the robot to succeed\neven on tasks where the pre-trained generalist policy fails entirely. Project page:\nhttps://demodiffusion.github.io/\nKeywords: Diffusion Policy, One-shot Imitation, Human to Robot\nFigure 1: Overview of DemoDiffusion. We show how generalist pre-trained diffusion policies can be used\nfor following a generic human demonstration showing a manipulation task during deployment. Our real-world\nmanipulation results encompass a wide diversity of manipulation tasks involving everyday objects.\n1 Introduction\nHow do we build robot manipulation systems that can be readily deployed in unstructured human\nenvironments? One possible answer is to learn \u2018generalist\u2019 policies that are capable of accomplishing",
  "2506.20633v1": "arXiv:2506.20633v1  [nlin.PS]  25 Jun 2025\nrd-spiral: An open-source Python library for learning\n2D reaction-di\ufb00usion dynamics through\npseudo-spectral method\nSandy H. S. Herho\nDepartment of Earth and Planetary Sciences,\nUniversity of California, Riverside, CA, USA 92521,\nsandy.herho@email.ucr.edu\nand\nIwan P. Anwar\nBandung Institute of Technology (ITB),\nOceanography Research Group,\nBandung, West Java, Indonesia, 40132\niwanpanwar@itb.ac.id\nand\nRusmawan Suwarman\nBandung Institute of Technology (ITB),\nAtmospheric Science Research Group,\nBandung, West Java, Indonesia, 40132\nrusmawan@itb.ac.id\n1",
  "2506.20629v1": "arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\nPLoP: Precise LoRA Placement for Efficient\nFinetuning of Large Models\nSoufiane Hayou\u2217\nSimons Institute\nUC Berkeley\nNikhil Ghosh\nFlatiron Institute\nBin Yu\nDept of Statistics\nUC Berkeley\nAbstract\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\nlarge models. Its small memory footprint allows practitioners to adapt\nlarge models to specific tasks at a fraction of the cost of full finetuning.\nDifferent modifications have been proposed to enhance its efficiency by,\nfor example, setting the learning rate, the rank, and the initialization. An-\nother improvement axis is adapter placement strategy: when using LoRA,\npractitioners usually pick module types to adapt with LoRA, such as Query\nand Key modules. Few works have studied the problem of adapter place-\nment, with nonconclusive results: original LoRA paper suggested placing\nadapters in attention modules, while other works suggested placing them\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\nautomatic identification of module types where LoRA adapters should be\nplaced, given a pretrained model and a finetuning task. We demonstrate\nthat PLoP consistently outperforms, and in the worst case competes, with\ncommonly used placement strategies through comprehensive experiments\non supervised finetuning and reinforcement learning for reasoning.\n\u2217Corresponding author: hayou@berkeley.edu",
  "2506.20600v1": "arXiv:2506.20600v1  [cs.AI]  25 Jun 2025\nCogGen: A Learner-Centered Generative AI\nArchitecture for Intelligent Tutoring with\nProgramming Videos\nWengxi Li1[0009\u22120003\u22121876\u22126303], Roy Pea2[0000\u22120001\u22126301\u22123536], Nick\nHaber2[0000\u22120001\u22128804\u22127804], and Hariharan Subramonyam2[0000\u22120002\u22123450\u22120447]\n1 City University of Hong Kong, Hong Kong SAR, China\nwengxili@cityu.edu.hk\n2 Stanford University, Stanford, California, United States\n{roypea,nhaber,harihars}@stanford.edu\nAbstract. We introduce CogGen, a learner-centered AI architecture\nthat transforms programming videos into interactive, adaptive learning\nexperiences by integrating student modeling with generative AI tutor-\ning based on the Cognitive Apprenticeship framework. The architecture\nconsists of three components: (1) video segmentation by learning goals,\n(2) a conversational tutoring engine applying Cognitive Apprenticeship\nstrategies, and (3) a student model using Bayesian Knowledge Tracing to\nadapt instruction. Our technical evaluation demonstrates effective video\nsegmentation accuracy and strong pedagogical alignment across knowl-\nedge, method, action, and interaction layers. Ablation studies confirm\nthe necessity of each component in generating effective guidance. This\nwork advances AI-powered tutoring by bridging structured student mod-\neling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education.\nKeywords: Cognitive Apprenticeship\u00b7 Large Langauge Models\u00b7 Stu-\ndent Modeling\u00b7 Conversational Agents\n1 Introduction\nVideo has emerged as a preferred medium for learning programming. Accord-\ning to a recent Stack Overflow survey, developers who learned through \u201cHow-\nto-videos\u201d and \u201cVideo-based E-Courses\u201d accounted for 54.2% and 49.9%, respec-\ntively3.Regardless,themosteffectivelearningparadigmforprogrammingvideos\nis learning by doing[4]. However, practicing coding tasks while watching videos\nis difficult for novices, requiring clear instructions and immediate feedback [1].\nLearning to program requires integrating multiple skills: understanding syn-\ntax,graspingprogrammingconcepts,andapplyingproblem-solvingstrategies[9].\nInstructional Design Principlesguide how these learning experiences should be\nstructured [7]. The Cognitive Apprenticeship (CogApp) framework addresses\n3 https://survey.stackoverflow.co/2024/developer-profile#learning-to-code"
}