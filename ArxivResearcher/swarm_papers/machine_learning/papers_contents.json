{
  "2506.20671v1": "arXiv:2506.20671v1  [cs.CV]  25 Jun 2025\nIPFormer: Visual 3D Panoptic Scene Completion\nwith Context-Adaptive Instance Proposals\nMarkus Gross1,2,\u2217\nAya Fahmy1 Danit Niwattananan2\nDominik Muhle2 Rui Song1,2 Daniel Cremers2 Henri Mee\u00df1\n1Fraunhofer Institute IVI 2Technical University of Munich\nAbstract\nSemantic Scene Completion (SSC) has emerged as a pivotal approach for jointly\nlearning scene geometry and semantics, enabling downstream applications such as\nnavigation in mobile robotics. The recent generalization to Panoptic Scene Com-\npletion (PSC) advances the SSC domain by integrating instance-level information,\nthereby enhancing object-level sensitivity in scene understanding. While PSC was\nintroduced using LiDAR modality, methods based on camera images remain largely\nunexplored. Moreover, recent Transformer-based SSC approaches utilize a fixed set\nof learned queries to reconstruct objects within the scene volume. Although these\nqueries are typically updated with image context during training, they remain static\nat test time, limiting their ability to dynamically adapt specifically to the observed\nscene. To overcome these limitations, we propose IPFormer, the first approach\nthat leverages context-adaptive instance proposals at train and test time to address\nvision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively\ninitializes these queries as panoptic instance proposals derived from image context\nand further refines them through attention-based encoding and decoding to reason\nabout semantic instance-voxel relationships. Experimental results show that our\napproach surpasses state-of-the-art methods in overall panoptic metrics PQ\u2020 and\nPQ-All, matches performance in individual metrics, and achieves a runtime reduc-\ntion exceeding 14\u00d7. Furthermore, our ablation studies reveal that dynamically\nderiving instance proposals from image context, as opposed to random initializa-\ntion, leads to a 3.62 %increase in PQ-All and a remarkable average improvement\nof 18.65 %in combined Thing-metrics. These results underscore the effectiveness\nof IPFormer and highlight its introduction of context-adaptive instance proposals\nas a pioneering effort in addressing vision-based 3D Panoptic Scene Completion.\n1 Introduction\nPanoptic scene completion provides a holistic scene understanding that can serve applications like\nautonomous driving and robotics by reconstructing volumetric geometry from sparse sensor data\nand assigning meaning to objects in the scene. Such holistic scene understanding requires both\naccurate geometry and semantic information about a scene. Historically, these tasks have been treated\nseparately and evolved independently due to the distinct nature of geometric reconstruction and\nsemantic interpretation, each requiring specialized algorithms and data representations. Despite the\ninherent interconnection between geometry and semantics, their separation hindered unified scene\nunderstanding. Classical structure-from-motion evolved into simultaneous localization and mapping\n(SLAM) systems for accurate reconstructions [36, 12, 25], while deep learning facilitated monocular\ndepth estimation (MDE) methods [14, 15, 50], which are restricted to visible surfaces. In contrast,\nscene completion refers to the process of inferring the complete 3D structure of a scene, including\nboth visible and occluded regions, from partial observations [49, 16].\n\u2217Corresponding author: markus.gross@tum.de",
  "2506.20670v1": "arXiv:2506.20670v1  [cs.CV]  25 Jun 2025\nMMSearch-R1: Incentivizing LMMs to Search\nJinming Wu1,\u2217, Zihao Deng1,\u2217, Wei Li1, Yiding Liu1, Bo You1,\nBo Li2,\u2020, Zejun Ma1, Ziwei Liu2\n1ByteDance 2S-Lab, NTU\nhttps://github.com/EvolvingLMMs-Lab/multimodal-search-r1\n<reason> The image search results mention the \"Viper\" lunar rover project and a \nnews article about the cancellation of the project. However, the image does not \nprovide information about the exact date when the project was canceled. To \nanswer the user's question, a text search is necessary to find the specific date of \nthe cancellation. </reason> <text_search> When was the NASA Viper moon \nrover project canceled in 2024? </text_search>\nIn which day of 2024 was \nthe lunar rover project in \nthe picture announced to \nbe canceled?\n<reason> The image shows a lunar rover, but there is no information about its \nannouncement or cancellation date. The image does not provide enough context \nto answer the user's question confidently.</reason> <search><img></search>\nImage Search: retrieve related webpages thumbnails and titles ...\nVIPER\n...\nText Search: retrieve related contents ...\n1. (link) NASA announced the cancellation of the VIPER (Volatiles \nInvestigating Polar Exploration Rover) project on July 17, 2024. The \ndecision followed a comprehensive internal review ...\n2. (link) The NASA VIPER (Volatiles Investigating Polar Exploration \nRover) moon rover project was canceled in July 2024. Following this \ncancellation ... \n  ...\n<reason> The search results indicate that the NASA VIPER lunar rover \nproject was canceled on July 17, 2024. The information is consistent across \nmultiple sources, suggesting that this is the correct date for the cancellation. \n</reason> <answer> July 17 </answer>\nAnswer \nwith\nSearch\nAnswer \nwithout\nSearch\nWhat traditional \nperformance art is \nbeing shown in the \nimage?\n<reason> The image shows a scene with performers dressed in \ntraditional Chinese costumes, likely engaged in a theatrical performance. \nThe presence of elaborate costumes and props, along with the presence \nof a child performer, suggests that this could be a form of Chinese opera, \nsuch as Peking Opera. Peking Opera is a classic form of Chinese theater \nknown for its elaborate costumes, face paint, and stylized acting. The \nscene appears to be a part of a Peking Opera performance. </reason>\n<answer> Peking Opera </answer>\n40\n45\n50\n55\n60\n65\n70\nAccuracy\nSearch Ratio (SR) (%)\n0\n20\n40\n60\n80\n100\nFVQA-test InfoSeek MMSearch SimpleVQA LiveVQA\nQwen2.5VL-7B(RAG) Qwen2.5VL-32B(RAG) MMSearch-R1-7B\nRAG SR\nMMSearch-R1 SR\nMMSearch-R1 vs. RAG: Results on Five VQA Tasks\nVirtual Intelligent Planetary \nExploration Rover, VIPER \nMobili... - NASA Science\nScientists slam 'indefensible' axing of \nNasa's $450m Viper moon rover | Nasa | \nThe Guardian\nFigure 1: Overview of MMSearch-R1. MMSearch-R1 learns to recognize the boundaries of its\nknowledge and perform on-demand search, significantly reducing the number of searches required\nwhile outperforming RAG-based models on knowledge-intensive and info-seeking VQA tasks.\nAbstract\nRobust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as retrieval-augmented\ngeneration (RAG) and prompt engineered search agents rely on rigid pipelines,\noften leading to inefficient or excessive search behaviors. We present MMSearch-\nR1, the first end-to-end reinforcement learning framework that enables LMMs to\nperform on-demand, multi-turn search in real-world Internet environments. Our\nframework integrates both image and text search tools, allowing the model to\nreason about when and how to invoke them guided by an outcome-based reward\nwith a search penalty. To support training, We collect a multimodal search VQA\ndataset through a semi-automated pipeline that covers diverse visual and textual\nknowledge needs and curate a search-balanced subset with both search-required and\nsearch-free samples, which proves essential for shaping efficient and on-demand\nsearch behavior. Extensive experiments on knowledge-intensive and info-seeking\nVQA tasks show that our model not only outperforms RAG-based baselines of\nthe same model size, but also matches the performance of a larger RAG-based\nmodel while reducing search calls by over 30%. We further analyze key empirical\nfindings to offer actionable insights for advancing research in multimodal search.\n\u2217 Equal Contribution; \u2020 Work collaborated with ByteDance\nPreprint.",
  "2506.20668v1": "arXiv:2506.20668v1  [cs.RO]  25 Jun 2025\nDemoDiffusion: One-Shot Human Imitation\nusing pre-trained Diffusion Policy\nSungjae Park, Homanga Bharadhwaj, Shubham Tulsiani\nCarnegie Mellon University\n{sungjae2, hbharadh, stulsian}@andrew.cmu.edu\nAbstract: We proposeDemoDiffusion, a simple and scalable method for enabling\nrobots to perform manipulation tasks in natural environments by imitating a sin-\ngle human demonstration. Our approach is based on two key insights. First, the\nhand motion in a human demonstration provides a useful prior for the robot\u2019s end-\neffector trajectory, which we can convert into a rough open-loop robot motion\ntrajectory via kinematic retargeting. Second, while this retargeted motion cap-\ntures the overall structure of the task, it may not align well with plausible robot\nactions in-context. To address this, we leverage a pre-trained generalist diffusion\npolicy to modify the trajectory, ensuring it both follows the human motion and re-\nmains within the distribution of plausible robot actions. Our approach avoids the\nneed for online reinforcement learning or paired human-robot data, enabling ro-\nbust adaptation to new tasks and scenes with minimal manual effort. Experiments\nin both simulation and real-world settings show that DemoDiffusion outperforms\nboth the base policy and the retargeted trajectory, enabling the robot to succeed\neven on tasks where the pre-trained generalist policy fails entirely. Project page:\nhttps://demodiffusion.github.io/\nKeywords: Diffusion Policy, One-shot Imitation, Human to Robot\nFigure 1: Overview of DemoDiffusion. We show how generalist pre-trained diffusion policies can be used\nfor following a generic human demonstration showing a manipulation task during deployment. Our real-world\nmanipulation results encompass a wide diversity of manipulation tasks involving everyday objects.\n1 Introduction\nHow do we build robot manipulation systems that can be readily deployed in unstructured human\nenvironments? One possible answer is to learn \u2018generalist\u2019 policies that are capable of accomplishing",
  "2506.20664v1": "arXiv:2506.20664v1  [cs.AI]  25 Jun 2025\nThe Decrypto Benchmark for Multi-Agent\nReasoning and Theory of Mind\nAndrei Lupu1,2, Timon Willi1,2, Jakob Foerster1,2\n1FAIR at Meta,2University of Oxford\nAs Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-\nagent scenarios, interacting with human users and other agents in cooperative and competitive settings.\nThis will require new reasoning skills, chief amongst them beingtheory of mind(ToM), or the ability\nto reason about the \u201cmental\u201d states of other agents. However, ToM and other multi-agent abilities\nin LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage,\nsaturation, and lack of interactivity. We thus proposeDecrypto, a game-based benchmark for\nmulti-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics\nand multi-agent reinforcement learning. It is designed to be aseasy as possible in all other dimensions,\neliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also\nthe first platform for designing interactive ToM experiments.\nWe validate the benchmark design through comprehensive empirical evaluations of frontier LLMs,\nrobustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities\nlag behind humans and simple word-embedding baselines. We then create variants of two classic\ncognitive science experiments withinDecrypto to evaluate three key ToM abilities. Surprisingly,\nwe find that state-of-the-art reasoning models are significantlyworse at those tasks than their older\ncounterparts. This demonstrates thatDecrypto addresses a crucial gap in current reasoning and\nToM evaluations, and paves the path towards better artificial agents.\nDate: June 26, 2025\nCorrespondence: alupu@meta.com\nCode and Data:https://github.com/facebookresearch/decrypto/\n1 Introduction\nReasoning abilities and agentic behaviour are currently at the forefront of large language model (LLM)\nresearch (Huang et al., 2023; Jaech et al., 2024; Guo et al., 2025), with early applications of LLM agents\nincluding software engineering (Jimenez et al., 2023) and web navigation (Yao et al., 2023). While those tasks\nare predominantly single-agent, real-world agentic applications will increasingly involve complex interactions\nwith many other actors, including business partners, teammates, users and other agents. Thus, the future of\nAI problems is irreduciblymulti-step, multi-agent, partially observable, andstochastic \u2013 a reality not captured\nby the existing suite of benchmarks.\nTo navigate these multi-agent scenarios, LLMs will requiretheory of mind(ToM), the general ability of\nmodelling the mental states of other agents (artificial or biological), including their knowledge, beliefs,\nintentions and abilities. However, LLM ToM remains poorly understood. Despite ToM being inherently\ntied to multi-agentinteractions, the majority of existing benchmarks lack interactivity and are restricted to\nvariants of the Sally-Anne task (Baron-Cohen et al., 1985). Furthermore, Hu et al. argue that many of those\nbenchmarks suffer from confounding factors and bias introduced when translating embodied scenarios to text.\nTo address the gap in ToM evaluations, we introduceDecrypto, a multi-agent reasoning and ToM benchmark\nbasedonthe award-winninglanguagegameofthe samename. Decryptoisapragmaticinference game(Degen,\n2023), where two agents must exchange secret messages while preventing a third party from intercepting them.\nWe designedDecrypto to be future-proof and aseasy as possible for LLMs in all other dimensions except\n1",
  "2506.20661v1": "arXiv:2506.20661v1  [quant-ph]  25 Jun 2025\nArchitectural mechanisms of a universal fault-tolerant quantum computer\nDolev Bluvstein1,\u2217, Alexandra A. Geim 1,\u2217, Sophie H. Li 1, Simon J. Evered 1, J. Pablo Bonilla Ataides 1,\nGefen Baranes1,2, Andi Gu 1, Tom Manovitz1, Muqing Xu 1, Marcin Kalinowski 1, Shayan Majidy 1,\nChristian Kokail3,1, Nishad Maskara1, Elias C. Trapp1, Luke M. Stewart 1, Simon Hollerith1, Hengyun Zhou1,4,\nMichael J. Gullans5, Susanne F. Yelin1, Markus Greiner1, Vladan Vuleti\u00b4 c2, Madelyn Cain1, and Mikhail D. Lukin 1\n1Department of Physics, Harvard University, Cambridge, MA 02138, USA\n2Department of Physics and Research Laboratory of Electronics,\nMassachusetts Institute of Technology, Cambridge, MA 02139, USA\n3ITAMP, Harvard-Smithsonian Center for Astrophysics, Cambridge, MA 02138, USA\n4QuEra Computing Inc., Boston, MA 02135, USA\n5Joint Center for Quantum Information and Computer Science,\nNIST/University of Maryland, College Park, Maryland 20742, USA\n\u2217 These authors contributed equally to this work.\nQuantum error correction (QEC) [1, 2] is believed to be essential for the realization of large-scale\nquantum computers [3, 4]. However, due to the complexity of operating on the encoded \u2018logical\u2019\nqubits [5, 6], understanding the physical principles for building fault-tolerant quantum devices and\ncombining them into efficient architectures is an outstanding scientific challenge. Here we utilize\nreconfigurable arrays of up to 448 neutral atoms to implement all key elements of a universal,\nfault-tolerant quantum processing architecture and experimentally explore their underlying working\nmechanisms. We first employ surface codes to study how repeated QEC suppresses errors [6, 7],\ndemonstrating 2 .14(13)x below-threshold performance in a four-round characterization circuit by\nleveraging atom loss detection and machine learning decoding [8, 9]. We then investigate logical\nentanglement using transversal gates and lattice surgery [10\u201312], and extend it to universal logic\nthrough transversal teleportation with 3D [[15,1,3]] codes [13, 14], enabling arbitrary-angle synthesis\nwith logarithmic overhead [5, 15]. Finally, we develop mid-circuit qubit re-use [16], increasing\nexperimental cycle rates by two orders of magnitude and enabling deep-circuit protocols with dozens\nof logical qubits and hundreds of logical teleportations [17\u201320] with [[7,1,3]] and high-rate [[16,6,4]]\ncodes while maintaining constant internal entropy. Our experiments reveal key principles for efficient\narchitecture design, involving the interplay between quantum logic & entropy removal, judiciously\nusing physical entanglement in logic gates & magic state generation, and leveraging teleportations\nfor universality & physical qubit reset. These results establish foundations for scalable, universal\nerror-corrected processing and its practical implementation with neutral atom systems.\nThe central challenge of quantum computation is its\ninherent sensitivity to errors [4, 21]. Whereas classical\ncomputers are composed of intrinsically robust digital\nbits stabilized by dissipation [22], quantum states are\nintrinsically analog objects that evolve in a continuous\nstate space, with coherent unitary evolution that does\nnot allow dissipation [21]. Remarkably, the discovery of\nquantum error correction (QEC) [2, 23, 24] provides a\nmethod to realize robust quantum computation. In this\napproach, entanglement between many physical qubits is\nused to encode error-corrected \u2018logical\u2019 qubits that can\nhave exponentially low, digitized errors while performing\narbitrary, analog-like computations [1, 5].\nWhile recent experiments provide clear examples\nwhere QEC works in practice [7, 11, 25], large-scale\nfault-tolerant quantum computation (FTQC) remains a\nformidable challenge. In essence, it requires one to dis-\nsipatively remove errors from the physical system, while\nsimultaneously performing arbitrary coherent manipula-\ntion of the encoded logical qubits. Such operation re-\nquires a broad range of components [26, 27], coupled\nwith many important hardware considerations and a\nwide range of QEC techniques [12, 28\u201331]. Implement-\ning and combining these techniques in practical systems,\nand understanding the scientific principles of resulting\nfault-tolerant architectures, is a highly complex task at\nthe frontier of quantum science.\nHere we experimentally implement the core building\nblocks for scalable, universal quantum computation and\nidentify and explore key mechanisms for efficient archi-\ntectures suitable for deep-circuit FTQC. Utilizing a quan-\ntum processor based on reconfigurable neutral atom ar-\nrays and leveraging key hardware upgrades, we study\nbelow-threshold error correction, fault-tolerant quantum\noperations, arbitrary unitary synthesis, and physical er-\nror removal during deep-circuit execution, altogether\nachieving all ingredients for scalable (i.e., logarithmic\noverhead) realization of arbitrary quantum algorithms\n[5]. Our results further advance th"
}