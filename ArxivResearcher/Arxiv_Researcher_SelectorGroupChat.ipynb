{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selector Group Chat\n",
    "\n",
    "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:\n",
    "1. The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. You can override the model by providing a custom selection function.\n",
    "2. The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.\n",
    "3. The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.\n",
    "4. When the conversation ends, the team returns the TaskResult containing the conversation history from this task.\n",
    "\n",
    "Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling reset()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "azure_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import SourceMatchTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "PAPER_DIR = \"../ArxivResearcher/sgc_papers\"\n",
    "\n",
    "# Create the user proxy agent.\n",
    "user_proxy_agent = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Use arxiv to find the papers \n",
    "def search_arxiv(topic: str, max_results: int = 5):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the latest articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "            return file_path  # If file exists, return the path without updating\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.entry_id.split('/')[-1] # Use the last part of the entry_id as a unique identifier\n",
    "        paper_ids.append(paper_id)\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper_id] = paper_info\n",
    "\n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json.dumps(papers_info, indent=2))\n",
    "    print(f\"Papers metadata saved to {file_path}\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Create the arxiv search agent.\n",
    "arxiv_search_agent = AssistantAgent(\n",
    "    name=\"arxiv_search\",\n",
    "    description=\"An agent that can retrieve research papers related to user input on arXiv \\\n",
    "        and save the metadata of the papers in a JSON file.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[search_arxiv],\n",
    "    system_message=\"You are an expert academic search assistant. \\\n",
    "        Your task is to find the most relevant and recent papers on arXiv \\\n",
    "        for given user input, henceforth called topic, and stores them in JSON file 'papers_info.json'.\"\n",
    ")\n",
    "# Create the content extractor agent that downloads the PDFs and extracts the content.\n",
    "def content_extractor(file_path: str):\n",
    "\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        papers_info = json.load(json_file)\n",
    "\n",
    "    papers_contents = {}\n",
    "    # Iterate through each paper's info and download the PDF\n",
    "    for paper_id, info in papers_info.items():\n",
    "        pdf_path = os.path.join(os.path.dirname(file_path), f\"{paper_id}.pdf\")\n",
    "        \n",
    "        # Download the PDF if it doesn't exist\n",
    "        if not os.path.exists(pdf_path):\n",
    "            paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "            paper.download_pdf(filename=pdf_path)\n",
    "\n",
    "        # Load the PDF and extract text\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Extract the raw text from the document\n",
    "        if documents:\n",
    "            papers_contents[paper_id] = documents[0].page_content[:5000]  # Limit to first 5000 characters for brevity\n",
    "        else:\n",
    "            papers_contents[paper_id] = \"No content found.\"\n",
    "\n",
    "    # Save the extracted contents to a JSON file\n",
    "    contents_file_path = os.path.join(os.path.dirname(file_path), \"papers_contents.json\")\n",
    "    with open(contents_file_path, \"w\") as contents_file:\n",
    "        json.dump(papers_contents, contents_file, indent=2)\n",
    "\n",
    "    return contents_file_path\n",
    "\n",
    "# Create the Content Extractor agent.\n",
    "info_extractor_agent = AssistantAgent(\n",
    "    name=\"content_extractor\",\n",
    "    description=\"An agent that extracts raw text from all the downloaded arxiv research papers \\\n",
    "    obtained from the arxiv search agent.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[content_extractor],\n",
    "    system_message=\"You are a research papers content extractor. \\\n",
    "        Read paper_id from papers_info.json. Download PDFs and extract the raw text content from each paper. \\\n",
    "        Ignore the papers whose content could not be extracted.\\\n",
    "        The extracted contents should be saved in a JSON file named 'papers_contents.json'.\\\n",
    "        Return the path to the JSON file.\"\n",
    ")\n",
    "\n",
    "# Function to generate full text from the extracted papers contents.\n",
    "def generate_full_text(content_file_path: str):\n",
    "    \"\"\"Combine the raw texts of the papers from the JSON file.\"\"\"\n",
    "    with open(content_file_path, \"r\") as json_file:\n",
    "        papers_contents = json.load(json_file)\n",
    "    full_text = \"\\n\".join(papers_contents.values())\n",
    "    print(f\"Read {len(papers_contents)} papers contents from {content_file_path}.\")\n",
    "    print(f\"Total characters in papers contents: {len(full_text)}\")\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Create the Research Summarizer agent.\n",
    "research_summarizer_agent = AssistantAgent(\n",
    "    name=\"research_summarizer\",\n",
    "    description=\"An agent that writes a single coherent summary on a topic from the top 5 research papers on arXiv.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[generate_full_text], \n",
    "    system_message=\"You are an expert academic researcher. \\\n",
    "        Your task is to write a single coherent summary relevant to the given topic in under 5000 words. \\\n",
    "        To do this, first generate full text from the contents of all papers by reading \\\n",
    "        the JSON file 'papers_contents.json' located in the topic's subdirectory inside the project directory. \\\n",
    "        The file contains the paper_ids and raw text content from each paper. \\\n",
    "        Focus on common themes and significant advancements. \\\n",
    "        Do not just summarize the papers individually, Always address the topic at hand.\"   \n",
    ")\n",
    "\n",
    "# Create the summary writer agent that writes the generated summary to a text file.\n",
    "def write_summary_to_file(summary: str, topic: str):\n",
    "    \"\"\"Write the generated summary to a text file named 'summary.txt' in the topic's subdirectory.\"\"\"\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    summary_file_path = os.path.join(path, \"summary.txt\")\n",
    "    \n",
    "    with open(summary_file_path, \"w\") as summary_file:\n",
    "        summary_file.write(summary)\n",
    "    \n",
    "    print(f\"Summary written to {summary_file_path}\")\n",
    "    return summary_file_path\n",
    "\n",
    "summary_writer_agent = AssistantAgent(\n",
    "    name=\"summary_writer\",\n",
    "    description=\"An agent that writes the generated summary to a text file.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[write_summary_to_file],\n",
    "    system_message=\"You are a summary writer. \\\n",
    "        Your task is to write the given summary to a text file named 'summary.txt' in the topic's subdirectory.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the planning agent\n",
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=azure_model_client,\n",
    "    system_message=f\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        user_proxy_agent: The user who provides the initial input from the console.\n",
    "        arxiv_search_agent: An agent that can retrieve research papers related to user input on arXiv and save the metadata of the papers in a JSON file.\n",
    "        info_extractor_agent: An agent that extracts raw text from all the downloaded arxiv research papers\n",
    "                            obtained from the arxiv search agent.\n",
    "        research_summarizer_agent: An agent that writes a single coherent summary on a topic from the top 5 research papers on arXiv.\n",
    "        summary_writer_agent: An agent that writes the generated summary to a text file.\n",
    "\n",
    "    You will follow this sequence:\n",
    "        arxiv_search_agent will retrieve top 5 research papers based on the initial user prompt and save their metadata in papers_info.json.\n",
    "        info_extractor_agent will extract the raw text from the downloaded papers and save it in papers_contents.json.\n",
    "        research_summarizer_agent will generate a coherent summary based on the contents of the papers.\n",
    "        summary_writer_agent will write the generated summary to a text file.\n",
    "        \n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the team\n",
    "Let’s create the team with two termination conditions: \n",
    "- TextMentionTermination to end the conversation when the User Proxy sends \"QUIT\"\n",
    "- SourceMatchTermination to limit the conversation to avoid infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"QUIT\")\n",
    "source_match_termination = SourceMatchTermination('summary_writer')\n",
    "termination = text_mention_termination | source_match_termination\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, user_proxy_agent, arxiv_search_agent, info_extractor_agent, \\\n",
    "     research_summarizer_agent, summary_writer_agent],\n",
    "    model_client=azure_model_client,\n",
    "    termination_condition=termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Task and Run the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (PlanningAgent) ----------\n",
      "Please provide the initial input or topic you would like to research.\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "LoRA\n",
      "---------- ToolCallRequestEvent (arxiv_search) ----------\n",
      "[FunctionCall(id='call_H6mRskFT5vSv2XrNlY1MfSqv', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')]\n",
      "Papers metadata saved to ../ArxivResearcher/sgc_papers/lora/papers_info.json\n",
      "---------- ToolCallExecutionEvent (arxiv_search) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_info.json', name='search_arxiv', call_id='call_H6mRskFT5vSv2XrNlY1MfSqv', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (arxiv_search) ----------\n",
      "../ArxivResearcher/sgc_papers/lora/papers_info.json\n",
      "---------- ToolCallRequestEvent (content_extractor) ----------\n",
      "[FunctionCall(id='call_1rNVqG67foHdc30VwBcd8DkN', arguments='{\"file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_info.json\"}', name='content_extractor')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- ToolCallExecutionEvent (content_extractor) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_contents.json', name='content_extractor', call_id='call_1rNVqG67foHdc30VwBcd8DkN', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (content_extractor) ----------\n",
      "../ArxivResearcher/sgc_papers/lora/papers_contents.json\n",
      "---------- TextMessage (arxiv_search) ----------\n",
      "The search for recent papers on \"LoRA\" has been completed. You can find the details stored in the file `papers_info.json`. If you need further information or want to extract specific content from the papers, please let me know!\n",
      "---------- TextMessage (content_extractor) ----------\n",
      "The extraction of paper contents has been completed successfully. The extracted content is saved in the JSON file named `papers_contents.json`. You can find the file at the following path:\n",
      "\n",
      "`../ArxivResearcher/sgc_papers/lora/papers_contents.json` \n",
      "\n",
      "If you need any further assistance, feel free to ask!\n",
      "---------- ToolCallRequestEvent (research_summarizer) ----------\n",
      "Read 5 papers contents from ../ArxivResearcher/sgc_papers/lora/papers_contents.json.\n",
      "Total characters in papers contents: 16078\n",
      "[FunctionCall(id='call_zVJ9ObzbNin9ZiLSZLetwzrE', arguments='{\"content_file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_contents.json\"}', name='generate_full_text')]\n",
      "---------- ToolCallExecutionEvent (research_summarizer) ----------\n",
      "[FunctionExecutionResult(content=\"arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\\nPLoP: Precise LoRA Placement for Efficient\\nFinetuning of Large Models\\nSoufiane Hayou∗\\nSimons Institute\\nUC Berkeley\\nNikhil Ghosh\\nFlatiron Institute\\nBin Yu\\nDept of Statistics\\nUC Berkeley\\nAbstract\\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\\nlarge models. Its small memory footprint allows practitioners to adapt\\nlarge models to specific tasks at a fraction of the cost of full finetuning.\\nDifferent modifications have been proposed to enhance its efficiency by,\\nfor example, setting the learning rate, the rank, and the initialization. An-\\nother improvement axis is adapter placement strategy: when using LoRA,\\npractitioners usually pick module types to adapt with LoRA, such as Query\\nand Key modules. Few works have studied the problem of adapter place-\\nment, with nonconclusive results: original LoRA paper suggested placing\\nadapters in attention modules, while other works suggested placing them\\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\\nautomatic identification of module types where LoRA adapters should be\\nplaced, given a pretrained model and a finetuning task. We demonstrate\\nthat PLoP consistently outperforms, and in the worst case competes, with\\ncommonly used placement strategies through comprehensive experiments\\non supervised finetuning and reinforcement learning for reasoning.\\n∗Corresponding author: hayou@berkeley.edu\\narXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\", name='generate_full_text', call_id='call_zVJ9ObzbNin9ZiLSZLetwzrE', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (research_summarizer) ----------\n",
      "arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\n",
      "PLoP: Precise LoRA Placement for Efficient\n",
      "Finetuning of Large Models\n",
      "Soufiane Hayou∗\n",
      "Simons Institute\n",
      "UC Berkeley\n",
      "Nikhil Ghosh\n",
      "Flatiron Institute\n",
      "Bin Yu\n",
      "Dept of Statistics\n",
      "UC Berkeley\n",
      "Abstract\n",
      "Low-Rank Adaptation (LoRA) is a widely used finetuning method for\n",
      "large models. Its small memory footprint allows practitioners to adapt\n",
      "large models to specific tasks at a fraction of the cost of full finetuning.\n",
      "Different modifications have been proposed to enhance its efficiency by,\n",
      "for example, setting the learning rate, the rank, and the initialization. An-\n",
      "other improvement axis is adapter placement strategy: when using LoRA,\n",
      "practitioners usually pick module types to adapt with LoRA, such as Query\n",
      "and Key modules. Few works have studied the problem of adapter place-\n",
      "ment, with nonconclusive results: original LoRA paper suggested placing\n",
      "adapters in attention modules, while other works suggested placing them\n",
      "in the MLP modules. Through an intuitive theoretical analysis, we intro-\n",
      "duce PLoP (Precise LoRA Placement), a lightweight method that allows\n",
      "automatic identification of module types where LoRA adapters should be\n",
      "placed, given a pretrained model and a finetuning task. We demonstrate\n",
      "that PLoP consistently outperforms, and in the worst case competes, with\n",
      "commonly used placement strategies through comprehensive experiments\n",
      "on supervised finetuning and reinforcement learning for reasoning.\n",
      "∗Corresponding author: hayou@berkeley.edu\n",
      "arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\n",
      "Radial Attention: O(n log n) Sparse Attention\n",
      "with Energy Decay for Long Video Generation\n",
      "Xingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\n",
      "Shuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\n",
      "Kelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\n",
      "MIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\n",
      "https://github.com/mit-han-lab/radial-attention\n",
      "“Nothing spreads without loss; every signal, every influence, every attention —\n",
      "decays with distance.” — Inspired by thermodynamic principles\n",
      "Pre-Trained\n",
      "LoRA-Tuned Pre-Trained\n",
      "Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \n",
      "purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\n",
      "Dense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \n",
      "(a) 117 Frames (Default Length)\n",
      "Dense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\n",
      "Dense Attention    Vision Reward: 0.133 \n",
      "Tuning Cost: 746 GPU Hours      Latency: 2895s\n",
      "Radial Attention (Ours)    Vision Reward: 0.134 \n",
      "Tuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\n",
      "(b) 509 Frames (4× Extension)\n",
      "Figure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\n",
      "Radial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\n",
      "comparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\n",
      "up inference by up to 3.7× versus dense attention.\n",
      "Abstract\n",
      "Recent advances in diffusion models have enabled high-quality video generation,\n",
      "but the additional temporal dimension significantly increases computational costs,\n",
      "making training and inference on long videos prohibitively expensive. In this\n",
      "paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\n",
      "diffusion models: post-softmax attention scores diminish as spatial and temporal\n",
      "distance between tokens increase, akin to the physical decay of signal or waves\n",
      "over space and time in nature. Motivated by this, we propose Radial Attention, a\n",
      "scalable sparse attention mechanism with O(n log n) complexity that translates\n",
      "energy decay into exponentially decaying compute density, which is significantly\n",
      "more efficient than standard O(n2) dense attention and more expressive than linear\n",
      "attention. Specifically, Radial Attention employs a simple, static attention mask\n",
      "where each token attends to spatially nearby tokens, with the attention window size\n",
      "∗indicates equal contributions.\n",
      "Preprint. Under review.\n",
      "arXiv:2506.19658v1  [cs.CV]  24 Jun 2025\n",
      "SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\n",
      "Guided Prompting\n",
      "Yang Xing\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Jiong Wu\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Yuheng Bu\n",
      "Department of Electrical\n",
      "& Computer Engineering\n",
      "University of Florida\n",
      "Kuang Gong\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Abstract—Although new vision foundation models such as Seg-\n",
      "ment Anything Model 2 (SAM2) have significantly enhanced\n",
      "zero-shot image segmentation capabilities, reliance on human-\n",
      "provided prompts poses significant challenges in adapting\n",
      "SAM2 to medical image segmentation tasks. Moreover, SAM2’s\n",
      "performance in medical image segmentation was limited by the\n",
      "domain shift issue, since it was originally trained on natural\n",
      "images and videos. To address these challenges, we proposed\n",
      "SAM2 with support-set guided prompting (SAM2-SGP), a\n",
      "framework that eliminated the need for manual prompts. The\n",
      "proposed model leveraged the memory mechanism of SAM2\n",
      "to generate pseudo-masks using image–mask pairs from a\n",
      "support set via a Pseudo-mask Generation (PMG) module.\n",
      "We further introduced a novel Pseudo-mask Attention (PMA)\n",
      "module, which used these pseudo-masks to automatically gen-\n",
      "erate bounding boxes and enhance localized feature extraction\n",
      "by guiding attention to relevant areas. Furthermore, a low-\n",
      "rank adaptation (LoRA) strategy was adopted to mitigate the\n",
      "domain shift issue. The proposed framework was evaluated\n",
      "on both 2D and 3D datasets across multiple medical imaging\n",
      "modalities, including fundus photography, X-ray, computed\n",
      "tomography (CT), magnetic resonance imaging (MRI), positron\n",
      "emission tomography (PET), and ultrasound. The results\n",
      "demonstrated a significant performance improvement over\n",
      "state-of-the-art models, such as nnUNet and SwinUNet, as well\n",
      "as foundation models, such as SAM2 and MedSAM2, under-\n",
      "scoring the effectiveness of the proposed approach. Our code is\n",
      "publicly available at https://github.com/astlian9/SAM Support.\n",
      "Index Terms —Auto-prompting, Fine-tuning, Foundation\n",
      "Model, Medical Image Segmentation, SAM2.\n",
      "1. Introduction\n",
      "Vision foundation models have demonstrated strong\n",
      "zero-shot capabilities across various applications, including\n",
      "medical image segmentation [1]. Their impressive gener-\n",
      "alizability and few-shot learning capabilities make them\n",
      "attractive for adapting to downstream tasks, offering a\n",
      "more efficient alternative to training task-specific models\n",
      "from scratch. The segment anything model (SAM) [2] is\n",
      "a recently developed visual foundation model designed for\n",
      "promptable image segmentation, pretrained on over 1 billion\n",
      "masks from 11 million natural images. Leveraging its large-\n",
      "scale training data and generalizable architecture, SAM ex-\n",
      "hibited strong zero-shot segmentation performance by using\n",
      "prompts as an extra input, such as a bounding box or\n",
      "positive and negative clicks, demonstrating exceptional gen-\n",
      "eralization ability and establishing a new benchmark across\n",
      "various segmentation tasks [3], [4], [5]. Recent works also\n",
      "demonstrated the strong performance of the SAM model\n",
      "when applied to downstream medical image segmentation\n",
      "tasks [6], [7], [8], [9], [10], [11].\n",
      "To extend these capabilities to more complex scenar-\n",
      "ios, the SAM2 model has been developed to expand the\n",
      "functionality of SAM to include video inputs [12]. This\n",
      "extension enabled SAM2 to process temporal sequences\n",
      "of images, making it suitable for tasks that required the\n",
      "understanding of spatial continuity over multiple frames.\n",
      "Fine-tuning it on specific tasks [13], [14], [15] and directly\n",
      "evaluating it on few-shot segmentation [16], [17], [18] are\n",
      "two ongoing research topics of SAM2 in medical image\n",
      "segmentation [19]. Although these SAM2-based methods\n",
      "required minimal or no training data, they still had notable\n",
      "limitations. Firstly, their performance remained highly de-\n",
      "pendent on user-provided high-quality instructions. Due to\n",
      "this limitation, recent work on the SAM2 model focused\n",
      "mainly on interactive medical image segmentation. Further-\n",
      "more, it was trained on natural images and videos and could\n",
      "face domain shift issues when applied to medical image\n",
      "segmentation tasks.\n",
      "To tackle the aforementioned limitations, we proposed\n",
      "a novel model, SAM2 with support-set guided prompting\n",
      "(SAM2-SGP), for medical image segmentation. By incor-\n",
      "porating in-context learning with the memory mechanism\n",
      "of SAM2, SAM2-SGP could automatically generate high-\n",
      "quality prompts based on support sets. Specifically, the\n",
      "proposed SAM2-SGP model included a novel generator\n",
      "adapted from SAM2’s memory mechanism to generate the\n",
      "pseudo-mask from image-mask pairs of the support set.\n",
      "These pseudo-masks were then used to compute bounding\n",
      "boxes, which could be used to generate prompt embeddings.\n",
      "   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \n",
      "I/ITSEC [2024] Paper No. 24322 Page 1 of 13 \n",
      "Airway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \n",
      "arXiv:2506.19072v1  [cs.CV]  23 Jun 2025\n",
      "HAWAII: Hierarchical Visual Knowledge Transfer for\n",
      "Efficient Vision-Language Models\n",
      "Yimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\n",
      "University of Waterloo\n",
      "Abstract\n",
      "Improving the visual understanding ability of vision-language models (VLMs) is\n",
      "crucial for enhancing their performance across various tasks. While using multiple\n",
      "pretrained visual experts has shown great promise, it often incurs significant compu-\n",
      "tational costs during training and inference. To address this challenge, we propose\n",
      "HAWAII, a novel framework that distills knowledge from multiple visual experts\n",
      "into a single vision encoder, enabling it to inherit the complementary strengths of\n",
      "several experts with minimal computational overhead. To mitigate conflicts among\n",
      "different teachers and switch between different teacher-specific knowledge, instead\n",
      "of using a fixed set of adapters for multiple teachers, we propose to use teacher-\n",
      "specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\n",
      "adapter is aligned with a specific teacher, avoiding noisy guidance during distil-\n",
      "lation. To enable efficient knowledge distillation, we propose fine-grained and\n",
      "coarse-grained distillation. At the fine-grained level, token importance scores are\n",
      "employed to emphasize the most informative tokens from each teacher adaptively.\n",
      "At the coarse-grained level, we summarize the knowledge from multiple teachers\n",
      "and transfer it to the student using a set of general-knowledge LoRA adapters with\n",
      "a router. Extensive experiments on various vision-language tasks demonstrate the\n",
      "superiority of HAWAII, compared to the popular open-source VLMs.\n",
      "1 Introduction\n",
      "Vision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\n",
      "modal inputs by combining the powerful language reasoning capabilities of pretrained large language\n",
      "models (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\n",
      "els [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\n",
      "or MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\n",
      "At the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\n",
      "rich visual features directly impacts the generation and reasoning capabilities of the VLM.\n",
      "Recent studies have shown that incorporating multiple vision experts improves performance by a\n",
      "large margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\n",
      "efficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\n",
      "experts during both training and inference, making them expensive and less practical for deployment,\n",
      "especially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\n",
      "growing interest in approaches that can retain the benefits of multiple vision experts while avoiding\n",
      "their substantial inference-time costs.\n",
      "Knowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\n",
      "model (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\n",
      "As a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\n",
      "into a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\n",
      "all teachers, enhancing visual understanding while only adding a small set of trainable parameters.\n",
      "Preprint. Under review.\n",
      "---------- ToolCallRequestEvent (summary_writer) ----------Summary written to ../ArxivResearcher/sgc_papers/lora/summary.txt\n",
      "\n",
      "[FunctionCall(id='call_4yx9GyUzrxd95fFNM62yoSrF', arguments='{\"summary\":\"This summary includes recent advancements in LoRA (Low-Rank Adaptation) methods and their applications in various fields, particularly large model finetuning and video generation. A notable paper, \\'PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models,\\' introduces a method for optimizing adapter placement in neural networks, showing improved results over traditional methods. Another significant contribution is \\'Radial Attention,\\' which enhances video generation efficiency by employing a sparse attention mechanism that reduces computational costs while maintaining quality. Additionally, \\'SAM2-SGP\\' presents a novel method for medical image segmentation that overcomes challenges posed by domain shifts, demonstrating substantial performance improvements. Lastly, \\'HAWAII\\' focuses on efficiently transferring knowledge across vision-language models using teacher-specific LoRA adapters, indicating advances in both performance and resource management for complex model architectures.\",\"topic\":\"LoRA\"}', name='write_summary_to_file')]\n",
      "---------- ToolCallExecutionEvent (summary_writer) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/summary.txt', name='write_summary_to_file', call_id='call_4yx9GyUzrxd95fFNM62yoSrF', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (summary_writer) ----------\n",
      "../ArxivResearcher/sgc_papers/lora/summary.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=288, completion_tokens=14), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 22, 925120, tzinfo=datetime.timezone.utc), content='Please provide the initial input or topic you would like to research.', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 23, 496769, tzinfo=datetime.timezone.utc), request_id='4c834075-73e5-495e-be25-865300fb859d', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 27, 67812, tzinfo=datetime.timezone.utc), content='LoRA', type='TextMessage'), ToolCallRequestEvent(source='arxiv_search', models_usage=RequestUsage(prompt_tokens=132, completion_tokens=22), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 28, 981379, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_H6mRskFT5vSv2XrNlY1MfSqv', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='arxiv_search', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 30, 455915, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_info.json', name='search_arxiv', call_id='call_H6mRskFT5vSv2XrNlY1MfSqv', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='arxiv_search', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 30, 456442, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/sgc_papers/lora/papers_info.json', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_H6mRskFT5vSv2XrNlY1MfSqv', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')], results=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_info.json', name='search_arxiv', call_id='call_H6mRskFT5vSv2XrNlY1MfSqv', is_error=False)]), ToolCallRequestEvent(source='content_extractor', models_usage=RequestUsage(prompt_tokens=161, completion_tokens=32), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 32, 939496, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_1rNVqG67foHdc30VwBcd8DkN', arguments='{\"file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_info.json\"}', name='content_extractor')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='content_extractor', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 53, 551617, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_contents.json', name='content_extractor', call_id='call_1rNVqG67foHdc30VwBcd8DkN', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='content_extractor', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 53, 551941, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/sgc_papers/lora/papers_contents.json', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_1rNVqG67foHdc30VwBcd8DkN', arguments='{\"file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_info.json\"}', name='content_extractor')], results=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/papers_contents.json', name='content_extractor', call_id='call_1rNVqG67foHdc30VwBcd8DkN', is_error=False)]), TextMessage(source='arxiv_search', models_usage=RequestUsage(prompt_tokens=202, completion_tokens=50), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 56, 474440, tzinfo=datetime.timezone.utc), content='The search for recent papers on \"LoRA\" has been completed. You can find the details stored in the file `papers_info.json`. If you need further information or want to extract specific content from the papers, please let me know!', type='TextMessage'), TextMessage(source='content_extractor', models_usage=RequestUsage(prompt_tokens=273, completion_tokens=68), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 53, 58, 962030, tzinfo=datetime.timezone.utc), content='The extraction of paper contents has been completed successfully. The extracted content is saved in the JSON file named `papers_contents.json`. You can find the file at the following path:\\n\\n`../ArxivResearcher/sgc_papers/lora/papers_contents.json` \\n\\nIf you need any further assistance, feel free to ask!', type='TextMessage'), ToolCallRequestEvent(source='research_summarizer', models_usage=RequestUsage(prompt_tokens=371, completion_tokens=33), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 1, 49723, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_zVJ9ObzbNin9ZiLSZLetwzrE', arguments='{\"content_file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_contents.json\"}', name='generate_full_text')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='research_summarizer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 1, 52286, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content=\"arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\\nPLoP: Precise LoRA Placement for Efficient\\nFinetuning of Large Models\\nSoufiane Hayou∗\\nSimons Institute\\nUC Berkeley\\nNikhil Ghosh\\nFlatiron Institute\\nBin Yu\\nDept of Statistics\\nUC Berkeley\\nAbstract\\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\\nlarge models. Its small memory footprint allows practitioners to adapt\\nlarge models to specific tasks at a fraction of the cost of full finetuning.\\nDifferent modifications have been proposed to enhance its efficiency by,\\nfor example, setting the learning rate, the rank, and the initialization. An-\\nother improvement axis is adapter placement strategy: when using LoRA,\\npractitioners usually pick module types to adapt with LoRA, such as Query\\nand Key modules. Few works have studied the problem of adapter place-\\nment, with nonconclusive results: original LoRA paper suggested placing\\nadapters in attention modules, while other works suggested placing them\\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\\nautomatic identification of module types where LoRA adapters should be\\nplaced, given a pretrained model and a finetuning task. We demonstrate\\nthat PLoP consistently outperforms, and in the worst case competes, with\\ncommonly used placement strategies through comprehensive experiments\\non supervised finetuning and reinforcement learning for reasoning.\\n∗Corresponding author: hayou@berkeley.edu\\narXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\", name='generate_full_text', call_id='call_zVJ9ObzbNin9ZiLSZLetwzrE', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='research_summarizer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 1, 52681, tzinfo=datetime.timezone.utc), content=\"arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\\nPLoP: Precise LoRA Placement for Efficient\\nFinetuning of Large Models\\nSoufiane Hayou∗\\nSimons Institute\\nUC Berkeley\\nNikhil Ghosh\\nFlatiron Institute\\nBin Yu\\nDept of Statistics\\nUC Berkeley\\nAbstract\\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\\nlarge models. Its small memory footprint allows practitioners to adapt\\nlarge models to specific tasks at a fraction of the cost of full finetuning.\\nDifferent modifications have been proposed to enhance its efficiency by,\\nfor example, setting the learning rate, the rank, and the initialization. An-\\nother improvement axis is adapter placement strategy: when using LoRA,\\npractitioners usually pick module types to adapt with LoRA, such as Query\\nand Key modules. Few works have studied the problem of adapter place-\\nment, with nonconclusive results: original LoRA paper suggested placing\\nadapters in attention modules, while other works suggested placing them\\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\\nautomatic identification of module types where LoRA adapters should be\\nplaced, given a pretrained model and a finetuning task. We demonstrate\\nthat PLoP consistently outperforms, and in the worst case competes, with\\ncommonly used placement strategies through comprehensive experiments\\non supervised finetuning and reinforcement learning for reasoning.\\n∗Corresponding author: hayou@berkeley.edu\\narXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\", type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_zVJ9ObzbNin9ZiLSZLetwzrE', arguments='{\"content_file_path\":\"../ArxivResearcher/sgc_papers/lora/papers_contents.json\"}', name='generate_full_text')], results=[FunctionExecutionResult(content=\"arXiv:2506.20629v1  [cs.LG]  25 Jun 2025\\nPLoP: Precise LoRA Placement for Efficient\\nFinetuning of Large Models\\nSoufiane Hayou∗\\nSimons Institute\\nUC Berkeley\\nNikhil Ghosh\\nFlatiron Institute\\nBin Yu\\nDept of Statistics\\nUC Berkeley\\nAbstract\\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\\nlarge models. Its small memory footprint allows practitioners to adapt\\nlarge models to specific tasks at a fraction of the cost of full finetuning.\\nDifferent modifications have been proposed to enhance its efficiency by,\\nfor example, setting the learning rate, the rank, and the initialization. An-\\nother improvement axis is adapter placement strategy: when using LoRA,\\npractitioners usually pick module types to adapt with LoRA, such as Query\\nand Key modules. Few works have studied the problem of adapter place-\\nment, with nonconclusive results: original LoRA paper suggested placing\\nadapters in attention modules, while other works suggested placing them\\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\\nautomatic identification of module types where LoRA adapters should be\\nplaced, given a pretrained model and a finetuning task. We demonstrate\\nthat PLoP consistently outperforms, and in the worst case competes, with\\ncommonly used placement strategies through comprehensive experiments\\non supervised finetuning and reinforcement learning for reasoning.\\n∗Corresponding author: hayou@berkeley.edu\\narXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\", name='generate_full_text', call_id='call_zVJ9ObzbNin9ZiLSZLetwzrE', is_error=False)]), ToolCallRequestEvent(source='summary_writer', models_usage=RequestUsage(prompt_tokens=3848, completion_tokens=188), metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 6, 211313, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_4yx9GyUzrxd95fFNM62yoSrF', arguments='{\"summary\":\"This summary includes recent advancements in LoRA (Low-Rank Adaptation) methods and their applications in various fields, particularly large model finetuning and video generation. A notable paper, \\'PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models,\\' introduces a method for optimizing adapter placement in neural networks, showing improved results over traditional methods. Another significant contribution is \\'Radial Attention,\\' which enhances video generation efficiency by employing a sparse attention mechanism that reduces computational costs while maintaining quality. Additionally, \\'SAM2-SGP\\' presents a novel method for medical image segmentation that overcomes challenges posed by domain shifts, demonstrating substantial performance improvements. Lastly, \\'HAWAII\\' focuses on efficiently transferring knowledge across vision-language models using teacher-specific LoRA adapters, indicating advances in both performance and resource management for complex model architectures.\",\"topic\":\"LoRA\"}', name='write_summary_to_file')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='summary_writer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 6, 213218, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/summary.txt', name='write_summary_to_file', call_id='call_4yx9GyUzrxd95fFNM62yoSrF', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='summary_writer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 26, 12, 54, 6, 213395, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/sgc_papers/lora/summary.txt', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_4yx9GyUzrxd95fFNM62yoSrF', arguments='{\"summary\":\"This summary includes recent advancements in LoRA (Low-Rank Adaptation) methods and their applications in various fields, particularly large model finetuning and video generation. A notable paper, \\'PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models,\\' introduces a method for optimizing adapter placement in neural networks, showing improved results over traditional methods. Another significant contribution is \\'Radial Attention,\\' which enhances video generation efficiency by employing a sparse attention mechanism that reduces computational costs while maintaining quality. Additionally, \\'SAM2-SGP\\' presents a novel method for medical image segmentation that overcomes challenges posed by domain shifts, demonstrating substantial performance improvements. Lastly, \\'HAWAII\\' focuses on efficiently transferring knowledge across vision-language models using teacher-specific LoRA adapters, indicating advances in both performance and resource management for complex model architectures.\",\"topic\":\"LoRA\"}', name='write_summary_to_file')], results=[FunctionExecutionResult(content='../ArxivResearcher/sgc_papers/lora/summary.txt', name='write_summary_to_file', call_id='call_4yx9GyUzrxd95fFNM62yoSrF', is_error=False)])], stop_reason=\"'summary_writer' answered\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream the messages to the console.\n",
    "await Console(team.run_stream())  \n",
    "#await team.reset()  # Reset the team for the next run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
