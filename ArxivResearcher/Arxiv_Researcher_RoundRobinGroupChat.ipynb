{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen RoundRobinGroupChat\n",
    "A team is a group of agents that work together to achieve a common goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client\n",
    "Using the model client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "azure_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "def search_arxiv(topic: str, max_results: int = 5):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the latest articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "\n",
    "    return list(papers)\n",
    "\n",
    "a = search_arxiv(\"machine learning\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2506.19852v1', '2506.19850v1', '2506.19847v1']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper.entry_id.split('/')[-1] for paper in a] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort(key=lambda x: x.published, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2025-06-24', '2025-06-24', '2025-06-24']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(paper.published.date()) for paper in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/pdf/2506.19852v1',\n",
       " 'http://arxiv.org/pdf/2506.19850v1',\n",
       " 'http://arxiv.org/pdf/2506.19847v1']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper.pdf_url for paper in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Xingyang Li',\n",
       " 'Muyang Li',\n",
       " 'Tianle Cai',\n",
       " 'Haocheng Xi',\n",
       " 'Shuo Yang',\n",
       " 'Yujun Lin',\n",
       " 'Lvmin Zhang',\n",
       " 'Songlin Yang',\n",
       " 'Jinbo Hu',\n",
       " 'Kelly Peng',\n",
       " 'Maneesh Agrawala',\n",
       " 'Ion Stoica',\n",
       " 'Kurt Keutzer',\n",
       " 'Song Han',\n",
       " 'Yuqi Wang',\n",
       " 'Xinghang Li',\n",
       " 'Wenxuan Wang',\n",
       " 'Junbo Zhang',\n",
       " 'Yingyan Li',\n",
       " 'Yuntao Chen',\n",
       " 'Xinlong Wang',\n",
       " 'Zhaoxiang Zhang',\n",
       " 'Zeju Qiu',\n",
       " 'Weiyang Liu',\n",
       " 'Adrian Weller',\n",
       " 'Bernhard Schölkopf']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[author.name for paper in a for author in paper.authors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Team\n",
    "\n",
    "RoundRobinGroupChat is a team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context.\n",
    "\n",
    "We create a team with two AssistantAgent and a TextMentionTermination condition that stops the team when a specific word is detected in the agent’s response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import SourceMatchTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "PAPER_DIR = \"../ArxivResearcher/papers\"\n",
    "\n",
    "# Create the user proxy agent.\n",
    "user_proxy_agent = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Use arxiv to find the papers \n",
    "def search_arxiv(topic: str, max_results: int = 5):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the latest articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "            return file_path  # If file exists, return the path without updating\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.entry_id.split('/')[-1] # Use the last part of the entry_id as a unique identifier\n",
    "        paper_ids.append(paper_id)\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper_id] = paper_info\n",
    "\n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json.dumps(papers_info, indent=2))\n",
    "    print(f\"Papers metadata saved to {file_path}\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Create the arxiv search agent.\n",
    "arxiv_search_agent = AssistantAgent(\n",
    "    name=\"arxiv_search\",\n",
    "    description=\"An agent that can retrieve research papers related to user input on arXiv \\\n",
    "        and save the metadata of the papers in a JSON file.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[search_arxiv],\n",
    "    system_message=\"You are an expert academic search assistant. \\\n",
    "        Your task is to find the most relevant and recent papers on arXiv \\\n",
    "        for given user input, henceforth called topic, and stores them in JSON file 'papers_info.json'.\"\n",
    ")\n",
    "# Create the content extractor agent that downloads the PDFs and extracts the content.\n",
    "def content_extractor(file_path: str):\n",
    "\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        papers_info = json.load(json_file)\n",
    "\n",
    "    papers_contents = {}\n",
    "    # Iterate through each paper's info and download the PDF\n",
    "    for paper_id, info in papers_info.items():\n",
    "        pdf_path = os.path.join(os.path.dirname(file_path), f\"{paper_id}.pdf\")\n",
    "        \n",
    "        # Download the PDF if it doesn't exist\n",
    "        if not os.path.exists(pdf_path):\n",
    "            paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "            paper.download_pdf(filename=pdf_path)\n",
    "\n",
    "        # Load the PDF and extract text\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Extract the raw text from the document\n",
    "        if documents:\n",
    "            papers_contents[paper_id] = documents[0].page_content[:5000]  # Limit to first 5000 characters for brevity\n",
    "        else:\n",
    "            papers_contents[paper_id] = \"No content found.\"\n",
    "\n",
    "    # Save the extracted contents to a JSON file\n",
    "    contents_file_path = os.path.join(os.path.dirname(file_path), \"papers_contents.json\")\n",
    "    with open(contents_file_path, \"w\") as contents_file:\n",
    "        json.dump(papers_contents, contents_file, indent=2)\n",
    "\n",
    "    return contents_file_path\n",
    "\n",
    "# Create the Content Extractor agent.\n",
    "info_extractor_agent = AssistantAgent(\n",
    "    name=\"content_extractor\",\n",
    "    description=\"An agent that extracts raw text from all the downloaded arxiv research papers \\\n",
    "    obtained from the arxiv search agent.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[content_extractor],\n",
    "    system_message=\"You are a research papers content extractor. \\\n",
    "        Read paper_id from papers_info.json. Download PDFs and extract the raw text content from each paper. \\\n",
    "        Ignore the papers whose content could not be extracted.\\\n",
    "        The extracted contents should be saved in a JSON file named 'papers_contents.json'.\\\n",
    "        Return the path to the JSON file.\"\n",
    ")\n",
    "\n",
    "# Function to generate full text from the extracted papers contents.\n",
    "def generate_full_text(content_file_path: str):\n",
    "    \"\"\"Combine the raw texts of the papers from the JSON file.\"\"\"\n",
    "    with open(content_file_path, \"r\") as json_file:\n",
    "        papers_contents = json.load(json_file)\n",
    "    full_text = \"\\n\".join(papers_contents.values())\n",
    "    print(f\"Read {len(papers_contents)} papers contents from {content_file_path}.\")\n",
    "    print(f\"Total characters in papers contents: {len(full_text)}\")\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Create the Research Summarizer agent.\n",
    "research_summarizer_agent = AssistantAgent(\n",
    "    name=\"research_summarizer\",\n",
    "    description=\"An agent that writes a single coherent summary on a topic from the top 5 research papers on arXiv.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[generate_full_text], \n",
    "    system_message=\"You are an expert academic researcher. \\\n",
    "        Your task is to write a single coherent summary relevant to the given topic in under 5000 words. \\\n",
    "        To do this, first generate full text from the contents of all papers by reading \\\n",
    "        the JSON file 'papers_contents.json' located in the topic's subdirectory inside the project directory. \\\n",
    "        The file contains the paper_ids and raw text content from each paper. \\\n",
    "        Focus on common themes and significant advancements. \\\n",
    "        Do not just summarize the papers individually, Always address the topic at hand.\"   \n",
    ")\n",
    "\n",
    "# Create the summary writer agent that writes the generated summary to a text file.\n",
    "def write_summary_to_file(summary: str, topic: str):\n",
    "    \"\"\"Write the generated summary to a text file named 'summary.txt' in the topic's subdirectory.\"\"\"\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    summary_file_path = os.path.join(path, \"summary.txt\")\n",
    "    \n",
    "    with open(summary_file_path, \"w\") as summary_file:\n",
    "        summary_file.write(summary)\n",
    "    \n",
    "    print(f\"Summary written to {summary_file_path}\")\n",
    "    return summary_file_path\n",
    "\n",
    "summary_writer_agent = AssistantAgent(\n",
    "    name=\"summary_writer\",\n",
    "    description=\"An agent that writes the generated summary to a text file.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[write_summary_to_file],\n",
    "    system_message=\"You are a summary writer. \\\n",
    "        Your task is to write the given summary to a text file named 'summary.txt' in the topic's subdirectory.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Termination Condition And A Round Robin Group Chat Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"QUIT\")\n",
    "source_match_termination = SourceMatchTermination('summary_writer')\n",
    "termination = text_mention_termination | source_match_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RoundRobin team with the all the agents defined above.\n",
    "team = RoundRobinGroupChat(\n",
    "    [user_proxy_agent, arxiv_search_agent, info_extractor_agent, \\\n",
    "     research_summarizer_agent, summary_writer_agent],\n",
    "    termination_condition=termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user_proxy) ----------\n",
      "LoRA\n",
      "---------- ToolCallRequestEvent (arxiv_search) ----------\n",
      "[FunctionCall(id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')]\n",
      "---------- ToolCallExecutionEvent (arxiv_search) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_info.json', name='search_arxiv', call_id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (arxiv_search) ----------\n",
      "../ArxivResearcher/papers/lora/papers_info.json\n",
      "---------- ToolCallRequestEvent (content_extractor) ----------\n",
      "[FunctionCall(id='call_tu72wcxSynmMpHX4AZ6MFJL4', arguments='{\"file_path\":\"../ArxivResearcher/papers/lora/papers_info.json\"}', name='content_extractor')]\n",
      "---------- ToolCallExecutionEvent (content_extractor) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_contents.json', name='content_extractor', call_id='call_tu72wcxSynmMpHX4AZ6MFJL4', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (content_extractor) ----------\n",
      "../ArxivResearcher/papers/lora/papers_contents.json\n",
      "---------- ToolCallRequestEvent (research_summarizer) ----------\n",
      "[FunctionCall(id='call_NTLrzlNYUAZSlkZUEA2jFRdk', arguments='{\"content_file_path\": \"../ArxivResearcher/papers/lora/papers_contents.json\"}', name='generate_full_text')]\n",
      "---------- ToolCallExecutionEvent (research_summarizer) ----------\n",
      "[FunctionExecutionResult(content=\"arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\\narXiv:2506.18902v2  [cs.AI]  24 Jun 2025\\njina-embeddings-v4: Universal Embeddings for\\nMultimodal Multilingual Retrieval\\nMichael Günther∗, Saba Sturua∗, Mohammad Kalim Akram∗,\\nIsabelle Mohr∗, Andrei Ungureanu∗, Bo Wang∗, Sedigheh Eslami, Scott Martens,\\nMaximilian Werk, Nan Wangand Han Xiao\\nJina AI GmbH, Prinzessinnenstraße 19, 10969, Berlin, Germany\\nresearch@jina.ai\\nAbstract\\nWe introduce jina-embeddings-v4, a 3.8\\nbillion parameter multimodal embedding model\\nthat unifies text and image representations\\nthrough a novel architecture supporting both\\nsingle-vector and multi-vector embeddings\\nin the late interaction style. The model incor-\\nporates task-specific Low-Rank Adaptation\\n(LoRA) adapters to optimize performance\\nacross diverse retrieval scenarios, including\\nquery-document retrieval, semantic text sim-\\nilarity, and code search. Comprehensive evalu-\\nations demonstrate thatjina-embeddings-v4\\nachieves state-of-the-art performance on both\\nsingle-modal and cross-modal retrieval tasks,\\nwith particular strength in processing visually\\nrich content such as tables, charts, diagrams,\\nand mixed-media formats. To facilitate\\nevaluation of this capability, we also introduce\\nJina-VDR, a novel benchmark specifically\\ndesigned for visually rich image retrieval.\\n1 Introduction\\nWe present jina-embeddings-v4, a multimodal\\nembedding model capable of processing text and\\nimage data to produce semantic embedding vectors\\nof varying lengths, optimized for a broad array\\nof applications. It incorporates optimized LoRA\\nadapters [Hu et al., 2022] for information retrieval\\nand semantic text similarity. An adapter is also\\nprovided for programming language embeddings,\\ntechnical question-answering, and natural language\\ncode retrieval. It also brings new functionality to\\nprocessing visually rich images (also called visual\\ndocuments), i.e., materials mixing texts and images,\\ncontaining tables, charts, diagrams, and other kinds\\nof common mixed media [Ding et al., 2024]. We\\nhave also developed Jina-VDR, a new multilingual,\\nmulti-domain benchmark suite for a broad range\\nof visual retrieval tasks, to evaluate the capabilities\\nof jina-embeddings-v4.\\n*Equal contribution.\\nWe discuss the challenges of developing a\\nmultimodal, multi-functional, state-of-the-art\\nembedding model capable of handling texts in a\\nvariety of languages, including computer coding\\nlanguages, images, and “visually rich” data. The\\nresulting model, jina-embeddings-v4, projects\\ninputs from all modalities into a unified semantic\\nspace, minimizing or eliminating the “modality\\ngap” that has troubled similar projects [Liang et al.,\\n2022a]. In addition, we introduce Jina-VDR, an\\nadvanced benchmark for images like screenshots\\nand scans of visually complex documents.\\nThe major contributions of this work are as\\nfollows:\\n• We introduce a unified multi-task learning\\nparadigm that jointly optimizes embedding\\nmodels to represent texts and images as single-\\nand multi-vector embeddings.\\n• Building on work done for\\njina-embeddings-v3, we train LoRA\\nextensions to enhance support for specific\\ndomains and task types, achieving results\\ncomparable to specialized models.\\n• We have made particularly strong progress in\\nhandling visually rich images, especially for\\ntasks outside of the existing ViDoRe bench-\\nmark [Faysse et al., 2025], which is limited to\\nquestion-answering. jina-embeddings-v4\\noutperforms other multimodal models by a sig-\\nnificant margin on this type of material and sup-\\nports a much more diverse set of use scenarios.\\n• We construct a multilingual, multi-domain\\nbenchmark for screenshot retrieval. In contrast\\nto other retrieval benchmarks (i.e., [Faysse\\net al., 2025, Xiao et al., 2025]) that focus on\\nquestion answering and OCR-related tasks,\\nwe expand the scope of visual document\\nbenchmarking to multilingual retrieval,\\nmore query types, and a much more diverse\", name='generate_full_text', call_id='call_NTLrzlNYUAZSlkZUEA2jFRdk', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (research_summarizer) ----------\n",
      "arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\n",
      "Radial Attention: O(n log n) Sparse Attention\n",
      "with Energy Decay for Long Video Generation\n",
      "Xingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\n",
      "Shuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\n",
      "Kelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\n",
      "MIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\n",
      "https://github.com/mit-han-lab/radial-attention\n",
      "“Nothing spreads without loss; every signal, every influence, every attention —\n",
      "decays with distance.” — Inspired by thermodynamic principles\n",
      "Pre-Trained\n",
      "LoRA-Tuned Pre-Trained\n",
      "Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \n",
      "purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\n",
      "Dense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \n",
      "(a) 117 Frames (Default Length)\n",
      "Dense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\n",
      "Dense Attention    Vision Reward: 0.133 \n",
      "Tuning Cost: 746 GPU Hours      Latency: 2895s\n",
      "Radial Attention (Ours)    Vision Reward: 0.134 \n",
      "Tuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\n",
      "(b) 509 Frames (4× Extension)\n",
      "Figure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\n",
      "Radial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\n",
      "comparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\n",
      "up inference by up to 3.7× versus dense attention.\n",
      "Abstract\n",
      "Recent advances in diffusion models have enabled high-quality video generation,\n",
      "but the additional temporal dimension significantly increases computational costs,\n",
      "making training and inference on long videos prohibitively expensive. In this\n",
      "paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\n",
      "diffusion models: post-softmax attention scores diminish as spatial and temporal\n",
      "distance between tokens increase, akin to the physical decay of signal or waves\n",
      "over space and time in nature. Motivated by this, we propose Radial Attention, a\n",
      "scalable sparse attention mechanism with O(n log n) complexity that translates\n",
      "energy decay into exponentially decaying compute density, which is significantly\n",
      "more efficient than standard O(n2) dense attention and more expressive than linear\n",
      "attention. Specifically, Radial Attention employs a simple, static attention mask\n",
      "where each token attends to spatially nearby tokens, with the attention window size\n",
      "∗indicates equal contributions.\n",
      "Preprint. Under review.\n",
      "arXiv:2506.19658v1  [cs.CV]  24 Jun 2025\n",
      "SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\n",
      "Guided Prompting\n",
      "Yang Xing\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Jiong Wu\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Yuheng Bu\n",
      "Department of Electrical\n",
      "& Computer Engineering\n",
      "University of Florida\n",
      "Kuang Gong\n",
      "J. Crayton Pruitt Family\n",
      "Department of\n",
      "Biomedical Engineering\n",
      "University of Florida\n",
      "Abstract—Although new vision foundation models such as Seg-\n",
      "ment Anything Model 2 (SAM2) have significantly enhanced\n",
      "zero-shot image segmentation capabilities, reliance on human-\n",
      "provided prompts poses significant challenges in adapting\n",
      "SAM2 to medical image segmentation tasks. Moreover, SAM2’s\n",
      "performance in medical image segmentation was limited by the\n",
      "domain shift issue, since it was originally trained on natural\n",
      "images and videos. To address these challenges, we proposed\n",
      "SAM2 with support-set guided prompting (SAM2-SGP), a\n",
      "framework that eliminated the need for manual prompts. The\n",
      "proposed model leveraged the memory mechanism of SAM2\n",
      "to generate pseudo-masks using image–mask pairs from a\n",
      "support set via a Pseudo-mask Generation (PMG) module.\n",
      "We further introduced a novel Pseudo-mask Attention (PMA)\n",
      "module, which used these pseudo-masks to automatically gen-\n",
      "erate bounding boxes and enhance localized feature extraction\n",
      "by guiding attention to relevant areas. Furthermore, a low-\n",
      "rank adaptation (LoRA) strategy was adopted to mitigate the\n",
      "domain shift issue. The proposed framework was evaluated\n",
      "on both 2D and 3D datasets across multiple medical imaging\n",
      "modalities, including fundus photography, X-ray, computed\n",
      "tomography (CT), magnetic resonance imaging (MRI), positron\n",
      "emission tomography (PET), and ultrasound. The results\n",
      "demonstrated a significant performance improvement over\n",
      "state-of-the-art models, such as nnUNet and SwinUNet, as well\n",
      "as foundation models, such as SAM2 and MedSAM2, under-\n",
      "scoring the effectiveness of the proposed approach. Our code is\n",
      "publicly available at https://github.com/astlian9/SAM Support.\n",
      "Index Terms —Auto-prompting, Fine-tuning, Foundation\n",
      "Model, Medical Image Segmentation, SAM2.\n",
      "1. Introduction\n",
      "Vision foundation models have demonstrated strong\n",
      "zero-shot capabilities across various applications, including\n",
      "medical image segmentation [1]. Their impressive gener-\n",
      "alizability and few-shot learning capabilities make them\n",
      "attractive for adapting to downstream tasks, offering a\n",
      "more efficient alternative to training task-specific models\n",
      "from scratch. The segment anything model (SAM) [2] is\n",
      "a recently developed visual foundation model designed for\n",
      "promptable image segmentation, pretrained on over 1 billion\n",
      "masks from 11 million natural images. Leveraging its large-\n",
      "scale training data and generalizable architecture, SAM ex-\n",
      "hibited strong zero-shot segmentation performance by using\n",
      "prompts as an extra input, such as a bounding box or\n",
      "positive and negative clicks, demonstrating exceptional gen-\n",
      "eralization ability and establishing a new benchmark across\n",
      "various segmentation tasks [3], [4], [5]. Recent works also\n",
      "demonstrated the strong performance of the SAM model\n",
      "when applied to downstream medical image segmentation\n",
      "tasks [6], [7], [8], [9], [10], [11].\n",
      "To extend these capabilities to more complex scenar-\n",
      "ios, the SAM2 model has been developed to expand the\n",
      "functionality of SAM to include video inputs [12]. This\n",
      "extension enabled SAM2 to process temporal sequences\n",
      "of images, making it suitable for tasks that required the\n",
      "understanding of spatial continuity over multiple frames.\n",
      "Fine-tuning it on specific tasks [13], [14], [15] and directly\n",
      "evaluating it on few-shot segmentation [16], [17], [18] are\n",
      "two ongoing research topics of SAM2 in medical image\n",
      "segmentation [19]. Although these SAM2-based methods\n",
      "required minimal or no training data, they still had notable\n",
      "limitations. Firstly, their performance remained highly de-\n",
      "pendent on user-provided high-quality instructions. Due to\n",
      "this limitation, recent work on the SAM2 model focused\n",
      "mainly on interactive medical image segmentation. Further-\n",
      "more, it was trained on natural images and videos and could\n",
      "face domain shift issues when applied to medical image\n",
      "segmentation tasks.\n",
      "To tackle the aforementioned limitations, we proposed\n",
      "a novel model, SAM2 with support-set guided prompting\n",
      "(SAM2-SGP), for medical image segmentation. By incor-\n",
      "porating in-context learning with the memory mechanism\n",
      "of SAM2, SAM2-SGP could automatically generate high-\n",
      "quality prompts based on support sets. Specifically, the\n",
      "proposed SAM2-SGP model included a novel generator\n",
      "adapted from SAM2’s memory mechanism to generate the\n",
      "pseudo-mask from image-mask pairs of the support set.\n",
      "These pseudo-masks were then used to compute bounding\n",
      "boxes, which could be used to generate prompt embeddings.\n",
      "   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \n",
      "I/ITSEC [2024] Paper No. 24322 Page 1 of 13 \n",
      "Airway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \n",
      "arXiv:2506.19072v1  [cs.CV]  23 Jun 2025\n",
      "HAWAII: Hierarchical Visual Knowledge Transfer for\n",
      "Efficient Vision-Language Models\n",
      "Yimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\n",
      "University of Waterloo\n",
      "Abstract\n",
      "Improving the visual understanding ability of vision-language models (VLMs) is\n",
      "crucial for enhancing their performance across various tasks. While using multiple\n",
      "pretrained visual experts has shown great promise, it often incurs significant compu-\n",
      "tational costs during training and inference. To address this challenge, we propose\n",
      "HAWAII, a novel framework that distills knowledge from multiple visual experts\n",
      "into a single vision encoder, enabling it to inherit the complementary strengths of\n",
      "several experts with minimal computational overhead. To mitigate conflicts among\n",
      "different teachers and switch between different teacher-specific knowledge, instead\n",
      "of using a fixed set of adapters for multiple teachers, we propose to use teacher-\n",
      "specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\n",
      "adapter is aligned with a specific teacher, avoiding noisy guidance during distil-\n",
      "lation. To enable efficient knowledge distillation, we propose fine-grained and\n",
      "coarse-grained distillation. At the fine-grained level, token importance scores are\n",
      "employed to emphasize the most informative tokens from each teacher adaptively.\n",
      "At the coarse-grained level, we summarize the knowledge from multiple teachers\n",
      "and transfer it to the student using a set of general-knowledge LoRA adapters with\n",
      "a router. Extensive experiments on various vision-language tasks demonstrate the\n",
      "superiority of HAWAII, compared to the popular open-source VLMs.\n",
      "1 Introduction\n",
      "Vision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\n",
      "modal inputs by combining the powerful language reasoning capabilities of pretrained large language\n",
      "models (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\n",
      "els [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\n",
      "or MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\n",
      "At the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\n",
      "rich visual features directly impacts the generation and reasoning capabilities of the VLM.\n",
      "Recent studies have shown that incorporating multiple vision experts improves performance by a\n",
      "large margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\n",
      "efficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\n",
      "experts during both training and inference, making them expensive and less practical for deployment,\n",
      "especially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\n",
      "growing interest in approaches that can retain the benefits of multiple vision experts while avoiding\n",
      "their substantial inference-time costs.\n",
      "Knowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\n",
      "model (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\n",
      "As a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\n",
      "into a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\n",
      "all teachers, enhancing visual understanding while only adding a small set of trainable parameters.\n",
      "Preprint. Under review.\n",
      "arXiv:2506.18902v2  [cs.AI]  24 Jun 2025\n",
      "jina-embeddings-v4: Universal Embeddings for\n",
      "Multimodal Multilingual Retrieval\n",
      "Michael Günther∗, Saba Sturua∗, Mohammad Kalim Akram∗,\n",
      "Isabelle Mohr∗, Andrei Ungureanu∗, Bo Wang∗, Sedigheh Eslami, Scott Martens,\n",
      "Maximilian Werk, Nan Wangand Han Xiao\n",
      "Jina AI GmbH, Prinzessinnenstraße 19, 10969, Berlin, Germany\n",
      "research@jina.ai\n",
      "Abstract\n",
      "We introduce jina-embeddings-v4, a 3.8\n",
      "billion parameter multimodal embedding model\n",
      "that unifies text and image representations\n",
      "through a novel architecture supporting both\n",
      "single-vector and multi-vector embeddings\n",
      "in the late interaction style. The model incor-\n",
      "porates task-specific Low-Rank Adaptation\n",
      "(LoRA) adapters to optimize performance\n",
      "across diverse retrieval scenarios, including\n",
      "query-document retrieval, semantic text sim-\n",
      "ilarity, and code search. Comprehensive evalu-\n",
      "ations demonstrate thatjina-embeddings-v4\n",
      "achieves state-of-the-art performance on both\n",
      "single-modal and cross-modal retrieval tasks,\n",
      "with particular strength in processing visually\n",
      "rich content such as tables, charts, diagrams,\n",
      "and mixed-media formats. To facilitate\n",
      "evaluation of this capability, we also introduce\n",
      "Jina-VDR, a novel benchmark specifically\n",
      "designed for visually rich image retrieval.\n",
      "1 Introduction\n",
      "We present jina-embeddings-v4, a multimodal\n",
      "embedding model capable of processing text and\n",
      "image data to produce semantic embedding vectors\n",
      "of varying lengths, optimized for a broad array\n",
      "of applications. It incorporates optimized LoRA\n",
      "adapters [Hu et al., 2022] for information retrieval\n",
      "and semantic text similarity. An adapter is also\n",
      "provided for programming language embeddings,\n",
      "technical question-answering, and natural language\n",
      "code retrieval. It also brings new functionality to\n",
      "processing visually rich images (also called visual\n",
      "documents), i.e., materials mixing texts and images,\n",
      "containing tables, charts, diagrams, and other kinds\n",
      "of common mixed media [Ding et al., 2024]. We\n",
      "have also developed Jina-VDR, a new multilingual,\n",
      "multi-domain benchmark suite for a broad range\n",
      "of visual retrieval tasks, to evaluate the capabilities\n",
      "of jina-embeddings-v4.\n",
      "*Equal contribution.\n",
      "We discuss the challenges of developing a\n",
      "multimodal, multi-functional, state-of-the-art\n",
      "embedding model capable of handling texts in a\n",
      "variety of languages, including computer coding\n",
      "languages, images, and “visually rich” data. The\n",
      "resulting model, jina-embeddings-v4, projects\n",
      "inputs from all modalities into a unified semantic\n",
      "space, minimizing or eliminating the “modality\n",
      "gap” that has troubled similar projects [Liang et al.,\n",
      "2022a]. In addition, we introduce Jina-VDR, an\n",
      "advanced benchmark for images like screenshots\n",
      "and scans of visually complex documents.\n",
      "The major contributions of this work are as\n",
      "follows:\n",
      "• We introduce a unified multi-task learning\n",
      "paradigm that jointly optimizes embedding\n",
      "models to represent texts and images as single-\n",
      "and multi-vector embeddings.\n",
      "• Building on work done for\n",
      "jina-embeddings-v3, we train LoRA\n",
      "extensions to enhance support for specific\n",
      "domains and task types, achieving results\n",
      "comparable to specialized models.\n",
      "• We have made particularly strong progress in\n",
      "handling visually rich images, especially for\n",
      "tasks outside of the existing ViDoRe bench-\n",
      "mark [Faysse et al., 2025], which is limited to\n",
      "question-answering. jina-embeddings-v4\n",
      "outperforms other multimodal models by a sig-\n",
      "nificant margin on this type of material and sup-\n",
      "ports a much more diverse set of use scenarios.\n",
      "• We construct a multilingual, multi-domain\n",
      "benchmark for screenshot retrieval. In contrast\n",
      "to other retrieval benchmarks (i.e., [Faysse\n",
      "et al., 2025, Xiao et al., 2025]) that focus on\n",
      "question answering and OCR-related tasks,\n",
      "we expand the scope of visual document\n",
      "benchmarking to multilingual retrieval,\n",
      "more query types, and a much more diverse\n",
      "---------- ToolCallRequestEvent (summary_writer) ----------\n",
      "Summary written to ../ArxivResearcher/papers/lora/summary.txt\n",
      "[FunctionCall(id='call_wSiKNRudX1LnCkvO2NsDcumi', arguments='{\"summary\": \"The recent advancements in deep learning have inspired the development of several innovative models, including Radial Attention for video generation, SAM2-SGP for medical image segmentation, and HAWAII for efficient vision-language models. Radial Attention introduces a sparse attention mechanism with O(n log n) complexity, significantly reducing latency and computational costs for long video tasks while maintaining quality. SAM2-SGP enhances the Segment Anything Model (SAM2) by integrating automatic prompt generation to address domain shift issues in medical imaging, achieving superior performance across various modalities. HAWAII employs a knowledge distillation approach to consolidate strengths from multiple vision experts into a single architecture, optimizing efficiency without sacrificing performance. Additionally, jina-embeddings-v4 provides a multimodal embedding framework that integrates text and image data, enhancing retrieval capabilities in diverse contexts.\", \"topic\": \"LoRA\"}', name='write_summary_to_file'), FunctionCall(id='call_NKr2PX3HMlYtfq9atfQxq90w', arguments='{\"summary\": \"This summary highlights recent research involving low-rank adaptation (LoRA) in various state-of-the-art models aimed at improving efficiency in computational tasks. The papers showcased include those on sparse attention mechanisms, specifically Radial Attention, that demonstrate significant performance improvements in video generation tasks by reducing latency while retaining high-quality outputs. Additionally, the SAM2-SGP model addresses limitations in medical image segmentation by eliminating the need for manual prompting, thus enhancing accuracy and efficiency in this application. Knowledge distillation is also a focus, seen in HAWAII\\'s method of merging knowledge from multiple visual experts to create a unified vision encoder, and jina-embeddings-v4, a universal embedding model that supports multimodal retrieval, showcases LoRA\\'s role in optimizing various retrieval tasks.\", \"topic\": \"LoRA\"}', name='write_summary_to_file')]\n",
      "---------- ToolCallExecutionEvent (summary_writer) ----------\n",
      "[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_wSiKNRudX1LnCkvO2NsDcumi', is_error=False), FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_NKr2PX3HMlYtfq9atfQxq90w', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (summary_writer) ----------\n",
      "../ArxivResearcher/papers/lora/summary.txt\n",
      "../ArxivResearcher/papers/lora/summary.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 39, 111588, tzinfo=datetime.timezone.utc), request_id='00b52541-0571-47cf-9fc0-6ef6a67457df', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 44, 286630, tzinfo=datetime.timezone.utc), content='LoRA', type='TextMessage'), ToolCallRequestEvent(source='arxiv_search', models_usage=RequestUsage(prompt_tokens=112, completion_tokens=22), metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 45, 733456, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='arxiv_search', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 47, 67231, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_info.json', name='search_arxiv', call_id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='arxiv_search', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 47, 67528, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/papers/lora/papers_info.json', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', arguments='{\"topic\":\"LoRA\",\"max_results\":5}', name='search_arxiv')], results=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_info.json', name='search_arxiv', call_id='call_ZTd7bkyfAQ2BWTnmR1sPfiEh', is_error=False)]), ToolCallRequestEvent(source='content_extractor', models_usage=RequestUsage(prompt_tokens=138, completion_tokens=29), metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 19, 47, 932779, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_tu72wcxSynmMpHX4AZ6MFJL4', arguments='{\"file_path\":\"../ArxivResearcher/papers/lora/papers_info.json\"}', name='content_extractor')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='content_extractor', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 10, 590511, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_contents.json', name='content_extractor', call_id='call_tu72wcxSynmMpHX4AZ6MFJL4', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='content_extractor', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 10, 590725, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/papers/lora/papers_contents.json', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_tu72wcxSynmMpHX4AZ6MFJL4', arguments='{\"file_path\":\"../ArxivResearcher/papers/lora/papers_info.json\"}', name='content_extractor')], results=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/papers_contents.json', name='content_extractor', call_id='call_tu72wcxSynmMpHX4AZ6MFJL4', is_error=False)]), ToolCallRequestEvent(source='research_summarizer', models_usage=RequestUsage(prompt_tokens=215, completion_tokens=46), metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 13, 564369, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_NTLrzlNYUAZSlkZUEA2jFRdk', arguments='{\"content_file_path\": \"../ArxivResearcher/papers/lora/papers_contents.json\"}', name='generate_full_text')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='research_summarizer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 13, 566768, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content=\"arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\\narXiv:2506.18902v2  [cs.AI]  24 Jun 2025\\njina-embeddings-v4: Universal Embeddings for\\nMultimodal Multilingual Retrieval\\nMichael Günther∗, Saba Sturua∗, Mohammad Kalim Akram∗,\\nIsabelle Mohr∗, Andrei Ungureanu∗, Bo Wang∗, Sedigheh Eslami, Scott Martens,\\nMaximilian Werk, Nan Wangand Han Xiao\\nJina AI GmbH, Prinzessinnenstraße 19, 10969, Berlin, Germany\\nresearch@jina.ai\\nAbstract\\nWe introduce jina-embeddings-v4, a 3.8\\nbillion parameter multimodal embedding model\\nthat unifies text and image representations\\nthrough a novel architecture supporting both\\nsingle-vector and multi-vector embeddings\\nin the late interaction style. The model incor-\\nporates task-specific Low-Rank Adaptation\\n(LoRA) adapters to optimize performance\\nacross diverse retrieval scenarios, including\\nquery-document retrieval, semantic text sim-\\nilarity, and code search. Comprehensive evalu-\\nations demonstrate thatjina-embeddings-v4\\nachieves state-of-the-art performance on both\\nsingle-modal and cross-modal retrieval tasks,\\nwith particular strength in processing visually\\nrich content such as tables, charts, diagrams,\\nand mixed-media formats. To facilitate\\nevaluation of this capability, we also introduce\\nJina-VDR, a novel benchmark specifically\\ndesigned for visually rich image retrieval.\\n1 Introduction\\nWe present jina-embeddings-v4, a multimodal\\nembedding model capable of processing text and\\nimage data to produce semantic embedding vectors\\nof varying lengths, optimized for a broad array\\nof applications. It incorporates optimized LoRA\\nadapters [Hu et al., 2022] for information retrieval\\nand semantic text similarity. An adapter is also\\nprovided for programming language embeddings,\\ntechnical question-answering, and natural language\\ncode retrieval. It also brings new functionality to\\nprocessing visually rich images (also called visual\\ndocuments), i.e., materials mixing texts and images,\\ncontaining tables, charts, diagrams, and other kinds\\nof common mixed media [Ding et al., 2024]. We\\nhave also developed Jina-VDR, a new multilingual,\\nmulti-domain benchmark suite for a broad range\\nof visual retrieval tasks, to evaluate the capabilities\\nof jina-embeddings-v4.\\n*Equal contribution.\\nWe discuss the challenges of developing a\\nmultimodal, multi-functional, state-of-the-art\\nembedding model capable of handling texts in a\\nvariety of languages, including computer coding\\nlanguages, images, and “visually rich” data. The\\nresulting model, jina-embeddings-v4, projects\\ninputs from all modalities into a unified semantic\\nspace, minimizing or eliminating the “modality\\ngap” that has troubled similar projects [Liang et al.,\\n2022a]. In addition, we introduce Jina-VDR, an\\nadvanced benchmark for images like screenshots\\nand scans of visually complex documents.\\nThe major contributions of this work are as\\nfollows:\\n• We introduce a unified multi-task learning\\nparadigm that jointly optimizes embedding\\nmodels to represent texts and images as single-\\nand multi-vector embeddings.\\n• Building on work done for\\njina-embeddings-v3, we train LoRA\\nextensions to enhance support for specific\\ndomains and task types, achieving results\\ncomparable to specialized models.\\n• We have made particularly strong progress in\\nhandling visually rich images, especially for\\ntasks outside of the existing ViDoRe bench-\\nmark [Faysse et al., 2025], which is limited to\\nquestion-answering. jina-embeddings-v4\\noutperforms other multimodal models by a sig-\\nnificant margin on this type of material and sup-\\nports a much more diverse set of use scenarios.\\n• We construct a multilingual, multi-domain\\nbenchmark for screenshot retrieval. In contrast\\nto other retrieval benchmarks (i.e., [Faysse\\net al., 2025, Xiao et al., 2025]) that focus on\\nquestion answering and OCR-related tasks,\\nwe expand the scope of visual document\\nbenchmarking to multilingual retrieval,\\nmore query types, and a much more diverse\", name='generate_full_text', call_id='call_NTLrzlNYUAZSlkZUEA2jFRdk', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='research_summarizer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 13, 567103, tzinfo=datetime.timezone.utc), content=\"arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\\narXiv:2506.18902v2  [cs.AI]  24 Jun 2025\\njina-embeddings-v4: Universal Embeddings for\\nMultimodal Multilingual Retrieval\\nMichael Günther∗, Saba Sturua∗, Mohammad Kalim Akram∗,\\nIsabelle Mohr∗, Andrei Ungureanu∗, Bo Wang∗, Sedigheh Eslami, Scott Martens,\\nMaximilian Werk, Nan Wangand Han Xiao\\nJina AI GmbH, Prinzessinnenstraße 19, 10969, Berlin, Germany\\nresearch@jina.ai\\nAbstract\\nWe introduce jina-embeddings-v4, a 3.8\\nbillion parameter multimodal embedding model\\nthat unifies text and image representations\\nthrough a novel architecture supporting both\\nsingle-vector and multi-vector embeddings\\nin the late interaction style. The model incor-\\nporates task-specific Low-Rank Adaptation\\n(LoRA) adapters to optimize performance\\nacross diverse retrieval scenarios, including\\nquery-document retrieval, semantic text sim-\\nilarity, and code search. Comprehensive evalu-\\nations demonstrate thatjina-embeddings-v4\\nachieves state-of-the-art performance on both\\nsingle-modal and cross-modal retrieval tasks,\\nwith particular strength in processing visually\\nrich content such as tables, charts, diagrams,\\nand mixed-media formats. To facilitate\\nevaluation of this capability, we also introduce\\nJina-VDR, a novel benchmark specifically\\ndesigned for visually rich image retrieval.\\n1 Introduction\\nWe present jina-embeddings-v4, a multimodal\\nembedding model capable of processing text and\\nimage data to produce semantic embedding vectors\\nof varying lengths, optimized for a broad array\\nof applications. It incorporates optimized LoRA\\nadapters [Hu et al., 2022] for information retrieval\\nand semantic text similarity. An adapter is also\\nprovided for programming language embeddings,\\ntechnical question-answering, and natural language\\ncode retrieval. It also brings new functionality to\\nprocessing visually rich images (also called visual\\ndocuments), i.e., materials mixing texts and images,\\ncontaining tables, charts, diagrams, and other kinds\\nof common mixed media [Ding et al., 2024]. We\\nhave also developed Jina-VDR, a new multilingual,\\nmulti-domain benchmark suite for a broad range\\nof visual retrieval tasks, to evaluate the capabilities\\nof jina-embeddings-v4.\\n*Equal contribution.\\nWe discuss the challenges of developing a\\nmultimodal, multi-functional, state-of-the-art\\nembedding model capable of handling texts in a\\nvariety of languages, including computer coding\\nlanguages, images, and “visually rich” data. The\\nresulting model, jina-embeddings-v4, projects\\ninputs from all modalities into a unified semantic\\nspace, minimizing or eliminating the “modality\\ngap” that has troubled similar projects [Liang et al.,\\n2022a]. In addition, we introduce Jina-VDR, an\\nadvanced benchmark for images like screenshots\\nand scans of visually complex documents.\\nThe major contributions of this work are as\\nfollows:\\n• We introduce a unified multi-task learning\\nparadigm that jointly optimizes embedding\\nmodels to represent texts and images as single-\\nand multi-vector embeddings.\\n• Building on work done for\\njina-embeddings-v3, we train LoRA\\nextensions to enhance support for specific\\ndomains and task types, achieving results\\ncomparable to specialized models.\\n• We have made particularly strong progress in\\nhandling visually rich images, especially for\\ntasks outside of the existing ViDoRe bench-\\nmark [Faysse et al., 2025], which is limited to\\nquestion-answering. jina-embeddings-v4\\noutperforms other multimodal models by a sig-\\nnificant margin on this type of material and sup-\\nports a much more diverse set of use scenarios.\\n• We construct a multilingual, multi-domain\\nbenchmark for screenshot retrieval. In contrast\\nto other retrieval benchmarks (i.e., [Faysse\\net al., 2025, Xiao et al., 2025]) that focus on\\nquestion answering and OCR-related tasks,\\nwe expand the scope of visual document\\nbenchmarking to multilingual retrieval,\\nmore query types, and a much more diverse\", type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_NTLrzlNYUAZSlkZUEA2jFRdk', arguments='{\"content_file_path\": \"../ArxivResearcher/papers/lora/papers_contents.json\"}', name='generate_full_text')], results=[FunctionExecutionResult(content=\"arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\\nRadial Attention: O(n log n) Sparse Attention\\nwith Energy Decay for Long Video Generation\\nXingyang Li∗ Muyang Li∗ Tianle Cai Haocheng Xi\\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\\nhttps://github.com/mit-han-lab/radial-attention\\n“Nothing spreads without loss; every signal, every influence, every attention —\\ndecays with distance.” — Inspired by thermodynamic principles\\nPre-Trained\\nLoRA-Tuned Pre-Trained\\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \\npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9× Faster)    PSNR: 27.3 \\n(a) 117 Frames (Default Length)\\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\\nDense Attention    Vision Reward: 0.133 \\nTuning Cost: 746 GPU Hours      Latency: 2895s\\nRadial Attention (Ours)    Vision Reward: 0.134 \\nTuning Cost: 171 GPU Hours (4.4× Fewer)     Latency: 781s (3.7× Faster)\\n(b) 509 Frames (4× Extension)\\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9× at its default video length while maintaining\\ncomparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds\\nup inference by up to 3.7× versus dense attention.\\nAbstract\\nRecent advances in diffusion models have enabled high-quality video generation,\\nbut the additional temporal dimension significantly increases computational costs,\\nmaking training and inference on long videos prohibitively expensive. In this\\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\\ndistance between tokens increase, akin to the physical decay of signal or waves\\nover space and time in nature. Motivated by this, we propose Radial Attention, a\\nscalable sparse attention mechanism with O(n log n) complexity that translates\\nenergy decay into exponentially decaying compute density, which is significantly\\nmore efficient than standard O(n2) dense attention and more expressive than linear\\nattention. Specifically, Radial Attention employs a simple, static attention mask\\nwhere each token attends to spatially nearby tokens, with the attention window size\\n∗indicates equal contributions.\\nPreprint. Under review.\\narXiv:2506.19658v1  [cs.CV]  24 Jun 2025\\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\\nGuided Prompting\\nYang Xing\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nJiong Wu\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nYuheng Bu\\nDepartment of Electrical\\n& Computer Engineering\\nUniversity of Florida\\nKuang Gong\\nJ. Crayton Pruitt Family\\nDepartment of\\nBiomedical Engineering\\nUniversity of Florida\\nAbstract—Although new vision foundation models such as Seg-\\nment Anything Model 2 (SAM2) have significantly enhanced\\nzero-shot image segmentation capabilities, reliance on human-\\nprovided prompts poses significant challenges in adapting\\nSAM2 to medical image segmentation tasks. Moreover, SAM2’s\\nperformance in medical image segmentation was limited by the\\ndomain shift issue, since it was originally trained on natural\\nimages and videos. To address these challenges, we proposed\\nSAM2 with support-set guided prompting (SAM2-SGP), a\\nframework that eliminated the need for manual prompts. The\\nproposed model leveraged the memory mechanism of SAM2\\nto generate pseudo-masks using image–mask pairs from a\\nsupport set via a Pseudo-mask Generation (PMG) module.\\nWe further introduced a novel Pseudo-mask Attention (PMA)\\nmodule, which used these pseudo-masks to automatically gen-\\nerate bounding boxes and enhance localized feature extraction\\nby guiding attention to relevant areas. Furthermore, a low-\\nrank adaptation (LoRA) strategy was adopted to mitigate the\\ndomain shift issue. The proposed framework was evaluated\\non both 2D and 3D datasets across multiple medical imaging\\nmodalities, including fundus photography, X-ray, computed\\ntomography (CT), magnetic resonance imaging (MRI), positron\\nemission tomography (PET), and ultrasound. The results\\ndemonstrated a significant performance improvement over\\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\\nas foundation models, such as SAM2 and MedSAM2, under-\\nscoring the effectiveness of the proposed approach. Our code is\\npublicly available at https://github.com/astlian9/SAM Support.\\nIndex Terms —Auto-prompting, Fine-tuning, Foundation\\nModel, Medical Image Segmentation, SAM2.\\n1. Introduction\\nVision foundation models have demonstrated strong\\nzero-shot capabilities across various applications, including\\nmedical image segmentation [1]. Their impressive gener-\\nalizability and few-shot learning capabilities make them\\nattractive for adapting to downstream tasks, offering a\\nmore efficient alternative to training task-specific models\\nfrom scratch. The segment anything model (SAM) [2] is\\na recently developed visual foundation model designed for\\npromptable image segmentation, pretrained on over 1 billion\\nmasks from 11 million natural images. Leveraging its large-\\nscale training data and generalizable architecture, SAM ex-\\nhibited strong zero-shot segmentation performance by using\\nprompts as an extra input, such as a bounding box or\\npositive and negative clicks, demonstrating exceptional gen-\\neralization ability and establishing a new benchmark across\\nvarious segmentation tasks [3], [4], [5]. Recent works also\\ndemonstrated the strong performance of the SAM model\\nwhen applied to downstream medical image segmentation\\ntasks [6], [7], [8], [9], [10], [11].\\nTo extend these capabilities to more complex scenar-\\nios, the SAM2 model has been developed to expand the\\nfunctionality of SAM to include video inputs [12]. This\\nextension enabled SAM2 to process temporal sequences\\nof images, making it suitable for tasks that required the\\nunderstanding of spatial continuity over multiple frames.\\nFine-tuning it on specific tasks [13], [14], [15] and directly\\nevaluating it on few-shot segmentation [16], [17], [18] are\\ntwo ongoing research topics of SAM2 in medical image\\nsegmentation [19]. Although these SAM2-based methods\\nrequired minimal or no training data, they still had notable\\nlimitations. Firstly, their performance remained highly de-\\npendent on user-provided high-quality instructions. Due to\\nthis limitation, recent work on the SAM2 model focused\\nmainly on interactive medical image segmentation. Further-\\nmore, it was trained on natural images and videos and could\\nface domain shift issues when applied to medical image\\nsegmentation tasks.\\nTo tackle the aforementioned limitations, we proposed\\na novel model, SAM2 with support-set guided prompting\\n(SAM2-SGP), for medical image segmentation. By incor-\\nporating in-context learning with the memory mechanism\\nof SAM2, SAM2-SGP could automatically generate high-\\nquality prompts based on support sets. Specifically, the\\nproposed SAM2-SGP model included a novel generator\\nadapted from SAM2’s memory mechanism to generate the\\npseudo-mask from image-mask pairs of the support set.\\nThese pseudo-masks were then used to compute bounding\\nboxes, which could be used to generate prompt embeddings.\\n   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \\nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \\nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket’s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  \\narXiv:2506.19072v1  [cs.CV]  23 Jun 2025\\nHAWAII: Hierarchical Visual Knowledge Transfer for\\nEfficient Vision-Language Models\\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\\nUniversity of Waterloo\\nAbstract\\nImproving the visual understanding ability of vision-language models (VLMs) is\\ncrucial for enhancing their performance across various tasks. While using multiple\\npretrained visual experts has shown great promise, it often incurs significant compu-\\ntational costs during training and inference. To address this challenge, we propose\\nHAWAII, a novel framework that distills knowledge from multiple visual experts\\ninto a single vision encoder, enabling it to inherit the complementary strengths of\\nseveral experts with minimal computational overhead. To mitigate conflicts among\\ndifferent teachers and switch between different teacher-specific knowledge, instead\\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\\nlation. To enable efficient knowledge distillation, we propose fine-grained and\\ncoarse-grained distillation. At the fine-grained level, token importance scores are\\nemployed to emphasize the most informative tokens from each teacher adaptively.\\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\\nand transfer it to the student using a set of general-knowledge LoRA adapters with\\na router. Extensive experiments on various vision-language tasks demonstrate the\\nsuperiority of HAWAII, compared to the popular open-source VLMs.\\n1 Introduction\\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\\nRecent studies have shown that incorporating multiple vision experts improves performance by a\\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\\nexperts during both training and inference, making them expensive and less practical for deployment,\\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\\ntheir substantial inference-time costs.\\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\\nPreprint. Under review.\\narXiv:2506.18902v2  [cs.AI]  24 Jun 2025\\njina-embeddings-v4: Universal Embeddings for\\nMultimodal Multilingual Retrieval\\nMichael Günther∗, Saba Sturua∗, Mohammad Kalim Akram∗,\\nIsabelle Mohr∗, Andrei Ungureanu∗, Bo Wang∗, Sedigheh Eslami, Scott Martens,\\nMaximilian Werk, Nan Wangand Han Xiao\\nJina AI GmbH, Prinzessinnenstraße 19, 10969, Berlin, Germany\\nresearch@jina.ai\\nAbstract\\nWe introduce jina-embeddings-v4, a 3.8\\nbillion parameter multimodal embedding model\\nthat unifies text and image representations\\nthrough a novel architecture supporting both\\nsingle-vector and multi-vector embeddings\\nin the late interaction style. The model incor-\\nporates task-specific Low-Rank Adaptation\\n(LoRA) adapters to optimize performance\\nacross diverse retrieval scenarios, including\\nquery-document retrieval, semantic text sim-\\nilarity, and code search. Comprehensive evalu-\\nations demonstrate thatjina-embeddings-v4\\nachieves state-of-the-art performance on both\\nsingle-modal and cross-modal retrieval tasks,\\nwith particular strength in processing visually\\nrich content such as tables, charts, diagrams,\\nand mixed-media formats. To facilitate\\nevaluation of this capability, we also introduce\\nJina-VDR, a novel benchmark specifically\\ndesigned for visually rich image retrieval.\\n1 Introduction\\nWe present jina-embeddings-v4, a multimodal\\nembedding model capable of processing text and\\nimage data to produce semantic embedding vectors\\nof varying lengths, optimized for a broad array\\nof applications. It incorporates optimized LoRA\\nadapters [Hu et al., 2022] for information retrieval\\nand semantic text similarity. An adapter is also\\nprovided for programming language embeddings,\\ntechnical question-answering, and natural language\\ncode retrieval. It also brings new functionality to\\nprocessing visually rich images (also called visual\\ndocuments), i.e., materials mixing texts and images,\\ncontaining tables, charts, diagrams, and other kinds\\nof common mixed media [Ding et al., 2024]. We\\nhave also developed Jina-VDR, a new multilingual,\\nmulti-domain benchmark suite for a broad range\\nof visual retrieval tasks, to evaluate the capabilities\\nof jina-embeddings-v4.\\n*Equal contribution.\\nWe discuss the challenges of developing a\\nmultimodal, multi-functional, state-of-the-art\\nembedding model capable of handling texts in a\\nvariety of languages, including computer coding\\nlanguages, images, and “visually rich” data. The\\nresulting model, jina-embeddings-v4, projects\\ninputs from all modalities into a unified semantic\\nspace, minimizing or eliminating the “modality\\ngap” that has troubled similar projects [Liang et al.,\\n2022a]. In addition, we introduce Jina-VDR, an\\nadvanced benchmark for images like screenshots\\nand scans of visually complex documents.\\nThe major contributions of this work are as\\nfollows:\\n• We introduce a unified multi-task learning\\nparadigm that jointly optimizes embedding\\nmodels to represent texts and images as single-\\nand multi-vector embeddings.\\n• Building on work done for\\njina-embeddings-v3, we train LoRA\\nextensions to enhance support for specific\\ndomains and task types, achieving results\\ncomparable to specialized models.\\n• We have made particularly strong progress in\\nhandling visually rich images, especially for\\ntasks outside of the existing ViDoRe bench-\\nmark [Faysse et al., 2025], which is limited to\\nquestion-answering. jina-embeddings-v4\\noutperforms other multimodal models by a sig-\\nnificant margin on this type of material and sup-\\nports a much more diverse set of use scenarios.\\n• We construct a multilingual, multi-domain\\nbenchmark for screenshot retrieval. In contrast\\nto other retrieval benchmarks (i.e., [Faysse\\net al., 2025, Xiao et al., 2025]) that focus on\\nquestion answering and OCR-related tasks,\\nwe expand the scope of visual document\\nbenchmarking to multilingual retrieval,\\nmore query types, and a much more diverse\", name='generate_full_text', call_id='call_NTLrzlNYUAZSlkZUEA2jFRdk', is_error=False)]), ToolCallRequestEvent(source='summary_writer', models_usage=RequestUsage(prompt_tokens=4237, completion_tokens=367), metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 26, 755211, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_wSiKNRudX1LnCkvO2NsDcumi', arguments='{\"summary\": \"The recent advancements in deep learning have inspired the development of several innovative models, including Radial Attention for video generation, SAM2-SGP for medical image segmentation, and HAWAII for efficient vision-language models. Radial Attention introduces a sparse attention mechanism with O(n log n) complexity, significantly reducing latency and computational costs for long video tasks while maintaining quality. SAM2-SGP enhances the Segment Anything Model (SAM2) by integrating automatic prompt generation to address domain shift issues in medical imaging, achieving superior performance across various modalities. HAWAII employs a knowledge distillation approach to consolidate strengths from multiple vision experts into a single architecture, optimizing efficiency without sacrificing performance. Additionally, jina-embeddings-v4 provides a multimodal embedding framework that integrates text and image data, enhancing retrieval capabilities in diverse contexts.\", \"topic\": \"LoRA\"}', name='write_summary_to_file'), FunctionCall(id='call_NKr2PX3HMlYtfq9atfQxq90w', arguments='{\"summary\": \"This summary highlights recent research involving low-rank adaptation (LoRA) in various state-of-the-art models aimed at improving efficiency in computational tasks. The papers showcased include those on sparse attention mechanisms, specifically Radial Attention, that demonstrate significant performance improvements in video generation tasks by reducing latency while retaining high-quality outputs. Additionally, the SAM2-SGP model addresses limitations in medical image segmentation by eliminating the need for manual prompting, thus enhancing accuracy and efficiency in this application. Knowledge distillation is also a focus, seen in HAWAII\\'s method of merging knowledge from multiple visual experts to create a unified vision encoder, and jina-embeddings-v4, a universal embedding model that supports multimodal retrieval, showcases LoRA\\'s role in optimizing various retrieval tasks.\", \"topic\": \"LoRA\"}', name='write_summary_to_file')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='summary_writer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 26, 759246, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_wSiKNRudX1LnCkvO2NsDcumi', is_error=False), FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_NKr2PX3HMlYtfq9atfQxq90w', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='summary_writer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 25, 20, 20, 26, 759473, tzinfo=datetime.timezone.utc), content='../ArxivResearcher/papers/lora/summary.txt\\n../ArxivResearcher/papers/lora/summary.txt', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_wSiKNRudX1LnCkvO2NsDcumi', arguments='{\"summary\": \"The recent advancements in deep learning have inspired the development of several innovative models, including Radial Attention for video generation, SAM2-SGP for medical image segmentation, and HAWAII for efficient vision-language models. Radial Attention introduces a sparse attention mechanism with O(n log n) complexity, significantly reducing latency and computational costs for long video tasks while maintaining quality. SAM2-SGP enhances the Segment Anything Model (SAM2) by integrating automatic prompt generation to address domain shift issues in medical imaging, achieving superior performance across various modalities. HAWAII employs a knowledge distillation approach to consolidate strengths from multiple vision experts into a single architecture, optimizing efficiency without sacrificing performance. Additionally, jina-embeddings-v4 provides a multimodal embedding framework that integrates text and image data, enhancing retrieval capabilities in diverse contexts.\", \"topic\": \"LoRA\"}', name='write_summary_to_file'), FunctionCall(id='call_NKr2PX3HMlYtfq9atfQxq90w', arguments='{\"summary\": \"This summary highlights recent research involving low-rank adaptation (LoRA) in various state-of-the-art models aimed at improving efficiency in computational tasks. The papers showcased include those on sparse attention mechanisms, specifically Radial Attention, that demonstrate significant performance improvements in video generation tasks by reducing latency while retaining high-quality outputs. Additionally, the SAM2-SGP model addresses limitations in medical image segmentation by eliminating the need for manual prompting, thus enhancing accuracy and efficiency in this application. Knowledge distillation is also a focus, seen in HAWAII\\'s method of merging knowledge from multiple visual experts to create a unified vision encoder, and jina-embeddings-v4, a universal embedding model that supports multimodal retrieval, showcases LoRA\\'s role in optimizing various retrieval tasks.\", \"topic\": \"LoRA\"}', name='write_summary_to_file')], results=[FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_wSiKNRudX1LnCkvO2NsDcumi', is_error=False), FunctionExecutionResult(content='../ArxivResearcher/papers/lora/summary.txt', name='write_summary_to_file', call_id='call_NKr2PX3HMlYtfq9atfQxq90w', is_error=False)])], stop_reason=\"'summary_writer' answered\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream the messages to the console.\n",
    "await Console(team.run_stream())  \n",
    "#await team.reset()  # Reset the team for the next run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aborting a Team\n",
    "You can abort a call to run() or run_stream() during execution by setting a CancellationToken passed to the cancellation_token parameter.\n",
    "\n",
    "Different from stopping a team, aborting a team will immediately stop the team and raise a CancelledError exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task was cancelled.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "# Create a cancellation token.\n",
    "cancellation_token = CancellationToken()\n",
    "\n",
    "# Use another coroutine to run the team.\n",
    "run = asyncio.create_task(\n",
    "    team.run(cancellation_token=cancellation_token,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cancel the run.\n",
    "cancellation_token.cancel()\n",
    "\n",
    "try:\n",
    "    result = await run  # This will raise a CancelledError.\n",
    "except asyncio.CancelledError:\n",
    "    print(\"Task was cancelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
