{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen RoundRobinGroupChat\n",
    "A team is a group of agents that work together to achieve a common goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client\n",
    "Using the model client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "azure_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Team\n",
    "\n",
    "RoundRobinGroupChat is a team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context.\n",
    "\n",
    "We create a team with two AssistantAgent and a TextMentionTermination condition that stops the team when a specific word is detected in the agent’s response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(papers)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Create the arxiv search agent.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m arxiv_search_agent = \u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marxiv_search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAn agent that searches for research papers related to \u001b[39;49m\u001b[38;5;132;43;01m{topic}\u001b[39;49;00m\u001b[33;43m on arXiv.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_model_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_arxiv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are an expert academic search engine. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[32m     42\u001b[39m \u001b[33;43m        Your task is to find the most relevant and recent papers on arXiv \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[32m     43\u001b[39m \u001b[33;43m        for a given topic \u001b[39;49m\u001b[38;5;132;43;01m{topic}\u001b[39;49;00m\u001b[33;43m and returning the results in python list.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_content\u001b[39m(topic, papers):\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Create directory for this topic\u001b[39;00m\n\u001b[32m     48\u001b[39m     path = os.path.join(PAPER_DIR, topic.lower().replace(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/azure-ai-agents/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:716\u001b[39m, in \u001b[36mAssistantAgent.__init__\u001b[39m\u001b[34m(self, name, model_client, tools, workbench, handoffs, model_context, description, system_message, model_client_stream, reflect_on_tool_use, tool_call_summary_format, tool_call_summary_formatter, output_content_type, output_content_type_format, memory, metadata)\u001b[39m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    715\u001b[39m         description = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28mself\u001b[39m._tools.append(\u001b[43mFunctionTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported tool type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tool)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/azure-ai-agents/.venv/lib/python3.12/site-packages/autogen_core/tools/_function_tool.py:98\u001b[39m, in \u001b[36mFunctionTool.__init__\u001b[39m\u001b[34m(self, func, description, name, global_imports, strict)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m._func = func\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m._global_imports = global_imports\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28mself\u001b[39m._signature = \u001b[43mget_typed_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m func_name = name \u001b[38;5;129;01mor\u001b[39;00m func.func.\u001b[34m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, functools.partial) \u001b[38;5;28;01melse\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m func.\u001b[34m__name__\u001b[39m\n\u001b[32m    100\u001b[39m args_model = args_base_model_from_signature(func_name + \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._signature)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/azure-ai-agents/.venv/lib/python3.12/site-packages/autogen_core/_function_utils.py:52\u001b[39m, in \u001b[36mget_typed_signature\u001b[39m\u001b[34m(call)\u001b[39m\n\u001b[32m     45\u001b[39m func_call = call.func \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(call, partial) \u001b[38;5;28;01melse\u001b[39;00m call\n\u001b[32m     46\u001b[39m type_hints = typing.get_type_hints(func_call, globalns, include_extras=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m typed_params = [\n\u001b[32m     48\u001b[39m     inspect.Parameter(\n\u001b[32m     49\u001b[39m         name=param.name,\n\u001b[32m     50\u001b[39m         kind=param.kind,\n\u001b[32m     51\u001b[39m         default=param.default,\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         annotation=\u001b[43mtype_hints\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m     53\u001b[39m     )\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m signature.parameters.values()\n\u001b[32m     55\u001b[39m ]\n\u001b[32m     56\u001b[39m return_annotation = type_hints.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m, inspect.Signature.empty)\n\u001b[32m     57\u001b[39m typed_signature = inspect.Signature(typed_params, return_annotation=return_annotation)\n",
      "\u001b[31mKeyError\u001b[39m: 'topic'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "PAPER_DIR = \"../ArxivResearcher/papers\"\n",
    "\n",
    "# Create the user proxy agent.\n",
    "user_proxy_agent = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "\n",
    "# Use arxiv to find the papers \n",
    "async def search_arxiv(topic, max_results=5):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the latest articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    papers = await client.results(search)\n",
    "\n",
    "    return list(papers)\n",
    "\n",
    "# Create the arxiv search agent.\n",
    "arxiv_search_agent = AssistantAgent(\n",
    "    name=\"arxiv_search\",\n",
    "    description=\"An agent that searches for research papers related to topic on arXiv.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[search_arxiv],\n",
    "    system_message=\"You are an expert academic search engine. \\\n",
    "        Your task is to find the most relevant and recent papers on arXiv \\\n",
    "        for a given topic and returning the results in python list.\"\n",
    ")\n",
    "\n",
    "async def extract_content(topic, papers):\n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, \"r\") as json_file:\n",
    "            contents = await json_file.read()\n",
    "            papers_info = json.loads(contents)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    async with aiofiles.open(file_path, \"w\") as json_file:\n",
    "        await json_file.write(json.dumps(papers_info, indent=2))\n",
    "\n",
    "    return f\"Saved paper metadata to: {file_path}. Paper IDs: {paper_ids}\"\n",
    "\n",
    "# Create the Content Extractor agent.\n",
    "info_extractor_agent = AssistantAgent(\n",
    "    name=\"content_extractor\",\n",
    "    description=\"An agent that extracts relevant information from research papers.\",\n",
    "    model_client=azure_model_client,\n",
    "    tools=[extract_content],\n",
    "    system_message=\"You are an expert in extracting relevant information from research papers. \\\n",
    "        Given a topic and a list of arxiv research papers, extract the title, authors, summary, pdf_url, and published date of each paper. \\\n",
    "        The file should be saved in a JSON file in a directory named after the topic, with spaces replaced by underscores. \\\n",
    "        The JSON file should be named 'papers_info.json'.\"\n",
    ")\n",
    "# Create the Research Summarizer agent.\n",
    "research_summarizer_agent = AssistantAgent(\n",
    "    name=\"research_summarizer\",\n",
    "    description=\"An agent that creates a concise summary on a topic from the given research papers.\",\n",
    "    model_client=azure_model_client,\n",
    "    system_message=\"You are an expert academic researcher. \\\n",
    "        The JSON file 'papers_info.json' will be located at {PAPER_DIR} and inside a subdirectory named after the topic, with spaces replaced by underscores. \\\n",
    "        The file contains the titles, authors, summaries, pdf_urls, and published dates of the papers. \\\n",
    "        Your task is to synthesize the main findings and contributions from multiple research paper \\\n",
    "        abstracts into a single, concise, and coherent overview of the topic. \\\n",
    "        Focus on common themes and significant advancements.\"    \n",
    ")\n",
    "\n",
    "# Define a termination condition that stops the task\n",
    "text_mention_termination = TextMentionTermination(\"QUIT\", \"STOP\", \"EXIT\", \"END\", \"CANCEL\")\n",
    "max_message_termination = MaxMessageTermination(max_message=20)\n",
    "termination = text_mention_termination | max_message_termination\n",
    "\n",
    "# Create a team with the arxiv search and context extractor agents.\n",
    "team = RoundRobinGroupChat(\n",
    "    [user_proxy_agent, arxiv_search_agent, info_extractor_agent, research_summarizer_agent],\n",
    "    termination_condition=termination\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Team\n",
    "Call the run() method to start the team with a task.\n",
    "\n",
    "The termination condition was met when the word “APPROVE” is detected in the agent’s response. When the team stops, it returns a TaskResult object with all the messages produced by the agents in the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 22, 20, 49, 59, 378389, tzinfo=datetime.timezone.utc), content='Write a short poem about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=28, completion_tokens=126), metadata={}, created_at=datetime.datetime(2025, 6, 22, 20, 50, 2, 62631, tzinfo=datetime.timezone.utc), content=\"In autumn's embrace, the leaves ignite,  \\nA tapestry of amber, gold, and bright.  \\nWhispers of wind through the branches sway,  \\nAs twilight descends, marking shorter days.  \\n\\nPumpkins abound in fields aglow,  \\nWhile harvest moon casts a gentle show.  \\nCrisp air carries scents of woodsmoke and spice,  \\nNature's farewell, so tenderly nice.  \\n\\nThe world transforms in a vibrant parade,  \\nWhere memories linger, and colors invade.  \\nIn the fall's soft dance, we find our grace,  \\nAs seasons shift, and time leaves its trace.  \", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=171, completion_tokens=196), metadata={}, created_at=datetime.datetime(2025, 6, 22, 20, 50, 3, 913875, tzinfo=datetime.timezone.utc), content='This poem beautifully captures the essence of autumn with vivid imagery and a rhythmic flow. The choice of words like \"tapestry,\" \"whispers,\" and \"aglow\" enhances the sensory experience, effectively invoking the sights, sounds, and smells of the season. \\n\\nHere are some constructive suggestions to enhance it further:\\n\\n1. **Variety in Rhythm**: Consider varying the meter in certain lines to create a more dynamic reading experience. This can help emphasize key moments or feelings.\\n\\n2. **Imagery Expansion**: While the imagery is strong, integrating more specific elements (like certain types of trees or animals) could deepen the connection to the season and create a more immersive experience for the reader.\\n\\n3. **Emotion Element**: Infusing a personal emotion or reflection related to fall could make the piece resonate more on a personal level.\\n\\nOverall, it’s a lovely portrayal of fall! Consider these suggestions, and once addressed, I\\'ll respond with \\'APPROVE\\'.', type='TextMessage')] stop_reason=\"Text 'APPROVE' mentioned\"\n"
     ]
    }
   ],
   "source": [
    "# Stream the messages to the console.\n",
    "await Console(team.run_stream(task=\"model context protocol\"))  \n",
    "#await team.reset()  # Reset the team for the next run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping a Team\n",
    "You can also stop the team from outside by using the ExternalTermination.\n",
    "\n",
    "Calling set() on ExternalTermination will stop the team when the current agent’s turn is over. \n",
    "\n",
    "Thus, the team may not stop immediately. This allows the current agent to finish its turn and broadcast the final message to the team before the team stops, keeping the team’s state consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination, ExternalTermination, TokenUsageTermination, TimeoutTermination, SourceMatchTermination, HandoffTermination, StopMessageTermination\\n\\n# MaxMessageTermination\\ntermination = MaxMessageTermination(5) # terminate after 5 messages\\nteam = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\\n\\nawait Console(team.run_stream(task=\"Give a number\"))\\n\\n#TextMentionTermination\\ntermination = TextMentionTermination(\"quit\") # Terminate when the message contains \"quit\".\\nteam = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\\nawait Console(team.run_stream(task=\"\"))\\n\\n#TokenUsageTermination\\ntermination = TokenUsageTermination(100) # terminate when token usage is met.\\nteam = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\\nawait Console(team.run_stream(task=\"Give a number\"))'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination, ExternalTermination, TokenUsageTermination, TimeoutTermination, SourceMatchTermination, HandoffTermination, StopMessageTermination\n",
    "\n",
    "# MaxMessageTermination\n",
    "termination = MaxMessageTermination(5) # terminate after 5 messages\n",
    "team = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\n",
    "\n",
    "await Console(team.run_stream(task=\"Give a number\"))\n",
    "\n",
    "#TextMentionTermination\n",
    "termination = TextMentionTermination(\"quit\") # Terminate when the message contains \"quit\".\n",
    "team = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\n",
    "await Console(team.run_stream(task=\"\"))\n",
    "\n",
    "#TokenUsageTermination\n",
    "termination = TokenUsageTermination(100) # terminate when token usage is met.\n",
    "team = RoundRobinGroupChat([first_agent, second_agent, third_agent], termination_condition=termination)\n",
    "await Console(team.run_stream(task=\"Give a number\"))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aborting a Team\n",
    "You can abort a call to run() or run_stream() during execution by setting a CancellationToken passed to the cancellation_token parameter.\n",
    "\n",
    "Different from stopping a team, aborting a team will immediately stop the team and raise a CancelledError exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task was cancelled.\n"
     ]
    }
   ],
   "source": [
    "# Create a cancellation token.\n",
    "cancellation_token = CancellationToken()\n",
    "\n",
    "# Use another coroutine to run the team.\n",
    "run = asyncio.create_task(\n",
    "    team.run(\n",
    "        task=\"model context protocol.\",\n",
    "        cancellation_token=cancellation_token,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cancel the run.\n",
    "cancellation_token.cancel()\n",
    "\n",
    "try:\n",
    "    result = await run  # This will raise a CancelledError.\n",
    "except asyncio.CancelledError:\n",
    "    print(\"Task was cancelled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
